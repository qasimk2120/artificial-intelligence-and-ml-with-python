{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fddb5ed2",
   "metadata": {},
   "source": [
    "# LangChain Expression Language (LCEL)\n",
    "\n",
    "This notebook demonstrates the **LangChain Expression Language (LCEL)**,\n",
    "which enables composing prompt templates, models, and output parsers\n",
    "into a single executable pipeline.\n",
    "\n",
    "The focus is on:\n",
    "- Building prompt templates with structured instructions\n",
    "- Parsing model output into deterministic formats\n",
    "- Composing components using the pipe (`|`) operator\n",
    "- Executing end-to-end chains in a single expression\n",
    "\n",
    "LCEL provides a concise and readable way to define LLM workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12bdb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser, StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f3b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191b55d6",
   "metadata": {},
   "source": [
    "## Output Format Instructions\n",
    "\n",
    "Before building the prompt, we define formatting instructions\n",
    "using an output parser.\n",
    "\n",
    "These instructions are injected into the prompt to ensure\n",
    "the model returns a predictable, structured response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1646c77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_intructions =  CommaSeparatedListOutputParser().get_format_instructions()\n",
    "list_intructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616083de",
   "metadata": {},
   "source": [
    "## Model Initialization\n",
    "\n",
    "A deterministic chat model is initialized to ensure\n",
    "consistent and parseable outputs when chaining components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863a31f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-5-nano\", \n",
    "    temperature=0, \n",
    "    model_kwargs= {\"text\":{\"verbosity\": 'low'},\"reasoning\":{\"effort\": \"medium\"}},\n",
    "    \n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77267f9",
   "metadata": {},
   "source": [
    "## Prompt Template Construction\n",
    "\n",
    "A `ChatPromptTemplate` is created with a dynamic placeholder\n",
    "and embedded formatting instructions.\n",
    "\n",
    "This template defines the structure of the user request\n",
    "without executing the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035e921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages([('human', \"I've recently adopted a {pet}, Can you suggest three {pet} names? \\n\" + list_intructions)])\n",
    "chat_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59ba4a7",
   "metadata": {},
   "source": [
    "## Manual Invocation and Parsing\n",
    "\n",
    "Before using LCEL, the prompt, model, and output parser\n",
    "can be invoked step by step to observe intermediate results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86ab228",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_output_parser =  CommaSeparatedListOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ce6023",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template_result = chat_template.invoke({\"pet\": \"dog\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea5a30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_result = chat.invoke(chat_template_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7430aa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_output_parser.invoke(chat_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb25da9",
   "metadata": {},
   "source": [
    "## Composing Chains with LCEL\n",
    "\n",
    "Using LCEL, individual components can be composed into a single\n",
    "executable chain using the pipe (`|`) operator.\n",
    "\n",
    "This allows the entire workflow to be expressed concisely\n",
    "in one readable line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593ccf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chat_template | chat | list_output_parser   # Create a chain by piping components together using expression language, all of the above process in one line\n",
    "chain.invoke({\"pet\": \"cat\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7eb0af",
   "metadata": {},
   "source": [
    "## Types of LangChain Objects That Can Be Integrated into a Chain\n",
    "\n",
    "- **Runnable** is the core abstraction in LangChain.  \n",
    "  A Runnable represents a unit of work that can be invoked, batched,\n",
    "  streamed, transformed, and composed with other Runnables.\n",
    "\n",
    "- **ChatPromptTemplate**, **Models**, and **Output Parsers**\n",
    "  are all implemented as Runnables.\n",
    "\n",
    "- **RunnableSequence** represents an ordered composition of Runnables.\n",
    "  Chains created with LCEL are RunnableSequences.\n",
    "\n",
    "- Since a RunnableSequence is itself a Runnable, chains can be composed\n",
    "  together to form longer and more complex pipelines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2647a9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "type(chain)\n",
    "type(chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d92efb",
   "metadata": {},
   "source": [
    "## RunnablePassthrough\n",
    "\n",
    "`RunnablePassthrough` acts as an identity Runnable.\n",
    "It forwards inputs without modification and is useful\n",
    "for wiring values between different parts of a chain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b977fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "RunnablePassthrough().invoke(\"This is a test string.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af957d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RunnablePassthrough().invoke([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316e4bee",
   "metadata": {},
   "source": [
    "## Chaining Chains Together\n",
    "\n",
    "Chains are themselves Runnables and can be composed\n",
    "to form longer pipelines.\n",
    "\n",
    "This enables multi-step reasoning workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7b8732",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template_tools = ChatPromptTemplate.from_template('''\n",
    "    What are the five most import tools a {job title} needs?\n",
    "    Answer only by listing the tools as a numbeered list\n",
    "''')\n",
    "\n",
    "chat_template_strategy = ChatPromptTemplate.from_template('''\n",
    "    Condisering the tools provided, develop a strategy for effectively learning and mastering them:\n",
    "    {tools}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3358129e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b19a6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef3f68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chain_strategy = chat_template_strategy | chat | string_parser\n",
    "chain_tools =  chat_template_tools | chat | string_parser | {'tools': RunnablePassthrough()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77eedf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain_tools.invoke({\"job title\": \"data scientist\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf84c2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain_strategy.invoke({'tools': '''\n",
    "    1. Python\n",
    "    2. SQL\n",
    "    3. R\n",
    "    4. Jupyter Notebook\n",
    "    5. Git\n",
    "'''}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1867c46",
   "metadata": {},
   "source": [
    "## Combining Chains into a Single Pipeline\n",
    "\n",
    "Previously defined chains can be composed together\n",
    "to form a complete end-to-end workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305275e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_combined = chain_tools | chain_strategy\n",
    "print(chain_combined.invoke({\"job title\": \"data scientist\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7510dbe1",
   "metadata": {},
   "source": [
    "## Long-Form LCEL Expression\n",
    "\n",
    "The same workflow can be written as a single LCEL expression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e96138",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_long = (chat_template_tools | chat | string_parser | {'tools': RunnablePassthrough()} | \n",
    "chat_template_strategy | chat | string_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8b0048",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain_long.invoke({\"job title\": \"data scientist\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c076568",
   "metadata": {},
   "source": [
    "## Visualizing Chain Execution Graphs\n",
    "\n",
    "LangChain provides a built-in way to visualize the execution graph\n",
    "of a chain. This visualization is powered internally by the\n",
    "**`grandalf`** graph layout library.\n",
    "\n",
    "The graph representation helps:\n",
    "- Understand the execution order of Runnables\n",
    "- Inspect how data flows between components\n",
    "- Debug complex LCEL pipelines\n",
    "- Reason about parallel and sequential execution\n",
    "\n",
    "Each node in the graph represents a Runnable, and edges represent\n",
    "data flow between them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc97ab34",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_long.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9249d010",
   "metadata": {},
   "source": [
    "## RunnableParallel\n",
    "\n",
    "`RunnableParallel` allows multiple Runnables to execute\n",
    "concurrently using the same input values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dea788",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template_books = ChatPromptTemplate.from_template(\n",
    "    '''\n",
    "    Suggest three of the best intermediate-level {programming language} books. \n",
    "    Answer only by listing the books.\n",
    "    '''\n",
    ")\n",
    "\n",
    "chat_template_projects = ChatPromptTemplate.from_template(\n",
    "    '''\n",
    "    Suggest three interesting {programming language} projects suitable for intermediate-level programmers. \n",
    "    Answer only by listing the projects.\n",
    "    '''\n",
    ")\n",
    "\n",
    "chat_template_time = ChatPromptTemplate.from_template(\n",
    "     '''\n",
    "     I'm an intermediate level programmer.\n",
    "     \n",
    "     Consider the following literature:\n",
    "     {books}\n",
    "     \n",
    "     Also, consider the following projects:\n",
    "     {projects}\n",
    "     \n",
    "     Roughly how much time would it take me to complete the literature and the projects?\n",
    "     \n",
    "     '''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bb5a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_books = chat_template_books | chat | string_parser\n",
    "chain_projects = chat_template_projects | chat | string_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e89fdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_parallel = RunnableParallel({'books': chain_books, 'projects': chain_projects})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c305983",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_parallel.invoke({'programming language': 'Python'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55659df5",
   "metadata": {},
   "source": [
    "## RunnableParallel Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853d1961",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_parallel.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f7ec8d",
   "metadata": {},
   "source": [
    "## Batch vs RunnableParallel\n",
    "\n",
    "- **Batch**: Same Runnable, multiple different inputs  \n",
    "- **RunnableParallel**: Different Runnables, same input  \n",
    "\n",
    "RunnableParallel is ideal when multiple tasks depend\n",
    "on the same input context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3307d59b",
   "metadata": {},
   "source": [
    "### Explicit vs Implicit `RunnableParallel` Syntax in LCEL\n",
    "\n",
    "LangChain Expression Language (LCEL) supports **two equivalent syntaxes**\n",
    "for parallel execution:\n",
    "\n",
    "1. **Explicit syntax** using `RunnableParallel`\n",
    "2. **Implicit syntax** using a dictionary of Runnables\n",
    "\n",
    "#### Explicit syntax\n",
    "```python\n",
    "RunnableParallel({'books': chain_books, 'projects': chain_projects})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fc1490",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_time1 = (RunnableParallel({'books': chain_books, 'projects': chain_projects})\n",
    "                                | chat_template_time \n",
    "                                | chat\n",
    "                                | string_parser\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e381a564",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_time2 = ({'books': chain_books, 'projects': chain_projects}\n",
    "                                | chat_template_time \n",
    "                                | chat\n",
    "                                | string_parser\n",
    "                                )     #this 2nd version without RunnableParallel but automatically parallelizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020b8e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain_time1.invoke({'programming language': 'Python'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e40f63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain_time2.invoke({'programming language': 'Python'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f030e268",
   "metadata": {},
   "source": [
    "## Final Chain Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15683b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_time1.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6438a344",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_time2.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7fae58",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "- Defining structured output instructions  \n",
    "- Creating reusable chat prompt templates  \n",
    "- Parsing model output into deterministic formats  \n",
    "- Composing prompt, model, and parser using LCEL  \n",
    "- Chaining chains together into longer pipelines  \n",
    "- Using RunnablePassthrough and RunnableParallel  \n",
    "- Visualizing execution graphs  \n",
    "\n",
    "LCEL enables expressive, composable, and scalable\n",
    "LLM application design.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchains_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
