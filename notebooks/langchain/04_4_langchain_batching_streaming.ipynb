{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ade14bd4",
   "metadata": {},
   "source": [
    "# Batching and Streaming with LangChain LCEL\n",
    "\n",
    "This notebook demonstrates **batching** and **streaming** capabilities\n",
    "in LangChain when using the **LangChain Expression Language (LCEL)**.\n",
    "\n",
    "The focus is on:\n",
    "- Building an LCEL chain using prompt templates and chat models\n",
    "- Comparing single invocation vs batch invocation\n",
    "- Measuring execution time differences\n",
    "- Streaming model output token-by-token\n",
    "\n",
    "These techniques are essential for building scalable\n",
    "and responsive LLM-powered applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dfbc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a308a610",
   "metadata": {},
   "source": [
    "## Prompt Template Definition\n",
    "\n",
    "A reusable `ChatPromptTemplate` is defined with dynamic placeholders\n",
    "for both the pet type and breed.\n",
    "\n",
    "This template will be reused across single, batch, and streaming calls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca48c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template  = ChatPromptTemplate.from_messages([('human', \"I've recently adopted a {pet} which is a {breed}, Can you suggest several training tips?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2382499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6e11e6",
   "metadata": {},
   "source": [
    "## Model Initialization\n",
    "\n",
    "A deterministic chat model is initialized to ensure\n",
    "consistent outputs when comparing different invocation methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a72308",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-5-nano\", \n",
    "    temperature=0, \n",
    "    model_kwargs= {\"text\":{\"verbosity\": 'low'},\"reasoning\":{\"effort\": \"medium\"}},\n",
    "    ) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158658bb",
   "metadata": {},
   "source": [
    "## LCEL Chain Construction\n",
    "\n",
    "Using the LangChain Expression Language, the prompt template\n",
    "and chat model are composed into a single executable chain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429866b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chat_template | chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78036cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"pet\": \"cat\", \"breed\": \"Siamese\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701d9092",
   "metadata": {},
   "source": [
    "## Batch Invocation\n",
    "\n",
    "Batching allows multiple inputs to be processed **in parallel**,\n",
    "which is more efficient than invoking the chain repeatedly.\n",
    "\n",
    "Key characteristics:\n",
    "- Parallel execution\n",
    "- Lower overhead per request\n",
    "- Ideal for bulk processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99df2f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time     \n",
    "chain.batch([{'pet':'cat', 'breed': 'Siamese'}, {'pet':'dragon', 'breed': 'night furry'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff294ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "chain.invoke({'pet':'dog', 'breed': 'shepherd'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad238ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "chain.invoke({'pet':'dragon', 'breed': 'night furry'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf683bef",
   "metadata": {},
   "source": [
    "## Streaming Responses\n",
    "\n",
    "Streaming allows tokens to be returned incrementally\n",
    "as they are generated by the model.\n",
    "\n",
    "This is useful for:\n",
    "- Interactive applications\n",
    "- Chat interfaces\n",
    "- Reducing perceived latency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c12b1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(   \n",
    "    model=\"gpt-4\",\n",
    "    temperature=0, \n",
    "    model_kwargs = {'seed': 365},\n",
    "    max_tokens =50\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60d4ea6",
   "metadata": {},
   "source": [
    "## Streaming Execution\n",
    "\n",
    "The `stream()` method returns an iterator over response chunks,\n",
    "which can be processed token-by-token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e060e62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.stream({'pet':'dragon', 'breed': 'night furry'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346fcdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3765bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in response:\n",
    "    print(chunk.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4062dd34",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "- Building an LCEL chain with prompt templates and chat models  \n",
    "- Executing single requests using `invoke()`  \n",
    "- Improving efficiency with parallel execution using `batch()`  \n",
    "- Streaming model output incrementally using `stream()`  \n",
    "\n",
    "Batching and streaming are key techniques for\n",
    "scalable, low-latency LLM applications.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchains_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
