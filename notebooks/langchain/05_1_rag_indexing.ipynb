{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cf4431f",
   "metadata": {},
   "source": [
    "# Steps in Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "This notebook demonstrates the **indexing phase** of a\n",
    "Retrieval-Augmented Generation (RAG) pipeline.\n",
    "\n",
    "The indexing phase prepares external knowledge so it can later\n",
    "be retrieved and injected into a language model during generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7160f0a5",
   "metadata": {},
   "source": [
    "## RAG Indexing Pipeline Overview\n",
    "\n",
    "The indexing stage of a RAG system consists of three core steps:\n",
    "\n",
    "1. **Document Loading**  \n",
    "   Load raw documents from different sources (PDF, DOCX, Markdown, etc.).\n",
    "\n",
    "2. **Document Splitting**  \n",
    "   Split documents into smaller, semantically meaningful chunks\n",
    "   suitable for embedding and retrieval.\n",
    "\n",
    "3. **Document Embedding and Storage**  \n",
    "   Convert text chunks into vector embeddings and store them\n",
    "   in a vector database for similarity search.\n",
    "\n",
    "This notebook focuses exclusively on these indexing steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88c4f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_chroma import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741c5f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c054f3",
   "metadata": {},
   "source": [
    "## Step 1: Document Loading\n",
    "\n",
    "Document loading is the first step in the RAG indexing pipeline.\n",
    "\n",
    "LangChain provides specialized loaders for handling different\n",
    "file formats while preserving metadata and document structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f920a1c",
   "metadata": {},
   "source": [
    "## Step 1: Document Loading\n",
    "\n",
    "Document loading is the first step in the RAG indexing pipeline.\n",
    "\n",
    "LangChain provides specialized loaders for handling different\n",
    "file formats while preserving metadata and document structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5849db3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_pdf = PyPDFLoader(\"../../data/docs/Introduction_to_Data_and_Data_Science.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d521a56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_pdf = loader_pdf.load()\n",
    "pages_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f051cfe7",
   "metadata": {},
   "source": [
    "PDF text often contains excessive whitespace and line breaks.\n",
    "To improve downstream processing, the page content is normalized\n",
    "by collapsing extra spaces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c5fd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_pdf_cut = copy.deepcopy(pages_pdf)  #to avoid modifying the original file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd7b501",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pages_pdf_cut:\n",
    "    i.page_content = ' '.join(i.page_content.split())\n",
    "\n",
    "pages_pdf_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888e0f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_pdf[0].page_content, pages_pdf_cut[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1f9615",
   "metadata": {},
   "source": [
    "### Loading Documents with `Docx2txtLoader`\n",
    "\n",
    "DOCX files are loaded using `Docx2txtLoader`,\n",
    "which extracts raw text from Word documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f98a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_docx = Docx2txtLoader(\"../../data/docs/Introduction_to_Data_and_Data_Science.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f723fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = loader_docx.load()\n",
    "for i in range(len(pages)):\n",
    "    pages[i].page_content = ' '.join(pages[i].page_content.split())\n",
    "\n",
    "pages[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb22280",
   "metadata": {},
   "source": [
    "## Step 2: Document Splitting\n",
    "\n",
    "Large documents must be split into smaller chunks before embedding.\n",
    "\n",
    "Smaller chunks:\n",
    "- Improve retrieval accuracy\n",
    "- Fit within model context limits\n",
    "- Preserve semantic coherence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faa74b5",
   "metadata": {},
   "source": [
    "### Character-Based Text Splitting\n",
    "\n",
    "Character-based splitting divides text using a fixed chunk size\n",
    "and optional overlap to preserve context between chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c030d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80f289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_splitter = CharacterTextSplitter(separator=\".\", chunk_size=500, chunk_overlap=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159e0abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_chat_split = char_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e21ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_chat_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e87518f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pages_chat_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae51ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pages_chat_split[16].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3d908f",
   "metadata": {},
   "source": [
    "### Markdown Header-Based Splitting\n",
    "\n",
    "When documents contain structured headers,\n",
    "a markdown-aware splitter preserves logical sections\n",
    "such as course titles and lecture headings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80c8665",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader2_docx = Docx2txtLoader(\"../../data/docs/Introduction_to_Data_and_Data_Science_2.docx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb5fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages2 = loader2_docx.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25de1a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc18171",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_splitter = MarkdownHeaderTextSplitter(headers_to_split_on = [(\"#\", \"Course Title\"), (\"##\", \"Lecture Title\")] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96417085",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_md_split = md_splitter.split_text(pages2[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35a2209",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pages_md_split)):\n",
    "    pages_md_split[i].page_content = ' '.join(pages_md_split[i].page_content.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b14258",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_char_split2 = char_splitter.split_documents(pages_md_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f71fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_char_split2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b201ac",
   "metadata": {},
   "source": [
    "## Step 3: Document Embedding and Storage\n",
    "\n",
    "After splitting, text chunks are converted into dense vector embeddings.\n",
    "These embeddings are stored in a vector database to enable\n",
    "efficient similarity search during retrieval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5d8884",
   "metadata": {},
   "source": [
    "### Generating Embeddings with OpenAI\n",
    "\n",
    "Each text chunk is mapped to a high-dimensional vector\n",
    "that captures semantic meaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b3a77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56984be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector1 = embeddings.embed_query(pages_char_split2[3].page_content)\n",
    "vector2 = embeddings.embed_query(pages_char_split2[5].page_content)\n",
    "vector3 = embeddings.embed_query(pages_char_split2[18].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e67e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vector1), len(vector2), len(vector3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c20ffc",
   "metadata": {},
   "source": [
    "### Measuring Similarity Between Embeddings\n",
    "\n",
    "Cosine similarity (dot product + vector norms)\n",
    "is commonly used to measure semantic similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8330bc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(vector1, vector2), np.dot(vector1, vector3), np.dot(vector2, vector3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f73a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(vector1), np.linalg.norm(vector2), np.linalg.norm(vector3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11663dbd",
   "metadata": {},
   "source": [
    "### Creating a Chroma Vector Store\n",
    "\n",
    "A vector store persists embeddings and enables\n",
    "efficient similarity-based retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89f29ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents= pages_char_split2, embedding=embeddings, persist_directory = \"./vectorstore/rag-practice\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d1280b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_from_directory = Chroma(persist_directory=\"./vectorstore/rag-practice\", embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab9b6ff",
   "metadata": {},
   "source": [
    "### Inspecting, Adding, Updating, and Deleting Documents\n",
    "\n",
    "Vector stores support full lifecycle management of documents,\n",
    "including insertion, updates, and deletion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e0af0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_from_directory.get(ids =\"123ef422-8ec3-4cd7-89ad-e095e77998fd\" , include=[\"embeddings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eececb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "added_document = Document(page_content='Alright! So… Let’s discuss the not-so-obvious differences between the terms analysis and analytics. Due to the similarity of the words, some people believe they share the same meaning, and thus use them interchangeably. Technically, this isn’t correct. There is, in fact, a distinct difference between the two. And the reason for one often being used instead of the other is the lack of a transparent understanding of both. So, let’s clear this up, shall we? First, we will start with analysis', \n",
    "                          metadata={'Course Title': 'Introduction to Data and Data Science', \n",
    "                                    'Lecture Title': 'Analysis vs Analytics'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1278d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_from_directory.add_documents([added_document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515a8a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_from_directory.get(\"9ef73267-b650-4011-82ec-025a21e1095d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9454daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_document = Document(page_content='Great! We hope we gave you a good idea about the level of applicability of the most frequently used programming and software tools in the field of data science. Thank you for watching!', \n",
    "                            metadata={'Course Title': 'Introduction to Data and Data Science', \n",
    "                                     'Lecture Title': 'Programming Languages & Software Employed in Data Science - All the Tools You Need'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406ea41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_from_directory.update_document(document_id=\"9ef73267-b650-4011-82ec-025a21e1095d\", document = updated_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27478bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_from_directory.get(\"9ef73267-b650-4011-82ec-025a21e1095d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047f66bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_from_directory.delete(\"9ef73267-b650-4011-82ec-025a21e1095d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418304d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_from_directory.get(\"9ef73267-b650-4011-82ec-025a21e1095d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6cb1f1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the **indexing phase of a RAG pipeline**:\n",
    "\n",
    "- Loading documents from PDF and DOCX sources  \n",
    "- Cleaning and normalizing raw text  \n",
    "- Splitting documents using structural and character-based strategies  \n",
    "- Generating semantic embeddings  \n",
    "- Storing and managing embeddings in a Chroma vector store  \n",
    "\n",
    "These steps form the foundation for retrieval-augmented\n",
    "generation systems.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (langchains_env)",
   "language": "python",
   "name": "langchains_env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
