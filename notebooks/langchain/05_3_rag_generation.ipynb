{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85a00a8c",
   "metadata": {},
   "source": [
    "# Generation in Retrieval-Augmented Generation (RAG): Stuffing Documents\n",
    "\n",
    "This notebook demonstrates the **generation phase** of a\n",
    "Retrieval-Augmented Generation (RAG) pipeline using the\n",
    "**document stuffing approach**.\n",
    "\n",
    "In this approach, retrieved documents are injected directly\n",
    "into the prompt context before generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386c9de6",
   "metadata": {},
   "source": [
    "## RAG Generation Overview\n",
    "\n",
    "After retrieval, the generation step produces a final answer\n",
    "by combining:\n",
    "\n",
    "- The user question\n",
    "- Retrieved document context\n",
    "- A structured prompt\n",
    "- A language model\n",
    "\n",
    "This notebook uses **prompt stuffing**, where all retrieved\n",
    "context is passed directly into the prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf22e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8f241c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c3b3b4",
   "metadata": {},
   "source": [
    "## Vector Store and Retriever\n",
    "\n",
    "The vector store created during the indexing phase is loaded\n",
    "from disk and wrapped as a retriever.\n",
    "\n",
    "The retriever selects the top-k most relevant document chunks\n",
    "for a given query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af5dc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma(persist_directory = \"./vectorstore/rag-practice\", \n",
    "                                    embedding_function = OpenAIEmbeddings(model=\"text-embedding-3-small\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7e71b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorstore.get()['documents'])  #check number of documents in the vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf2ea50",
   "metadata": {},
   "source": [
    "### Retriever Configuration\n",
    "\n",
    "The retriever is configured to return the top-2\n",
    "most relevant document chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8a7053",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver = vectorstore.as_retriever( search_kwargs={ \"k\":2,  } )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791781c3",
   "metadata": {},
   "source": [
    "## Prompt Template for Document Stuffing\n",
    "\n",
    "A prompt template is defined that:\n",
    "\n",
    "- Receives the user question\n",
    "- Injects retrieved context\n",
    "- Restricts the model to using only the provided context\n",
    "- Requires citation of source lectures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814ee7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = '''\n",
    "Answer the following question:\n",
    "{question}\n",
    "\n",
    "To answer the question, use only the following context:\n",
    "{context}\n",
    "\n",
    "At the end of the response, specify the name of the lecture this context is taken from in the format:\n",
    "Resources: *Lecture Title*\n",
    "where *Lecture Title* should be substituted with the title of all resource lectures.\n",
    "'''\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb709df",
   "metadata": {},
   "source": [
    "## Language Model Initialization\n",
    "\n",
    "A deterministic chat model is used to ensure\n",
    "stable and reproducible answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c522992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-5-nano\", \n",
    "    temperature=0, \n",
    "    model_kwargs= {\"text\":{\"verbosity\": 'low'},\"reasoning\":{\"effort\": \"medium\"}},\n",
    "    ) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7388e5d3",
   "metadata": {},
   "source": [
    "## Preparing the RAG Generation Chain\n",
    "\n",
    "The chain is constructed using LCEL:\n",
    "\n",
    "- The retriever supplies the context\n",
    "- `RunnablePassthrough` forwards the user question\n",
    "- The prompt template performs document stuffing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202910a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What software do data scientists use?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29a5af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = {'context': retriver,\n",
    "         'question': RunnablePassthrough()} | prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8e3a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(question)\n",
    "print(\"Generated Response:\\n\", chain.invoke(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a42c093",
   "metadata": {},
   "source": [
    "## Generating a Response\n",
    "\n",
    "The full RAG generation chain adds:\n",
    "\n",
    "- A chat model\n",
    "- An output parser to extract plain text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d129de",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = ({'context': retriver,\n",
    "         'question': RunnablePassthrough()} | prompt_template | chat | StrOutputParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85233557",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Generated Response:\\n\", chain.invoke(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c7c499",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the **generation phase** of a\n",
    "Retrieval-Augmented Generation (RAG) pipeline using document stuffing:\n",
    "\n",
    "- Loading a persisted vector store\n",
    "- Retrieving relevant document chunks\n",
    "- Injecting retrieved context into a structured prompt\n",
    "- Generating a grounded answer using an LLM\n",
    "\n",
    "Document stuffing is a simple and effective approach,\n",
    "but must be used carefully to avoid context length limits.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (langchains_env)",
   "language": "python",
   "name": "langchains_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
