{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "703eb15d",
   "metadata": {},
   "source": [
    "# Short-Term Memory in LangGraph with `InMemorySaver` and `SqliteSaver`\n",
    "\n",
    "This notebook demonstrates how **short-term memory** can be implemented\n",
    "in **LangGraph** using checkpointing.\n",
    "\n",
    "The focus is on:\n",
    "- Preserving conversational state across graph executions\n",
    "- Summarizing long conversations to control context length\n",
    "- Comparing in-memory vs SQLite-backed persistence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed962f0e",
   "metadata": {},
   "source": [
    "## Memory in LangGraph\n",
    "\n",
    "LangGraph supports **checkpointing**, which allows graph state to be:\n",
    "- Stored between node executions\n",
    "- Recovered after interruptions\n",
    "- Scoped to individual conversation threads\n",
    "\n",
    "Two checkpointing strategies are demonstrated:\n",
    "- `InMemorySaver` (ephemeral, session-bound)\n",
    "- `SqliteSaver` (persistent, disk-backed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c57e720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langgraph.graph import START, END, StateGraph, MessagesState\n",
    "from langchain_core.messages import HumanMessage, BaseMessage, AIMessage, RemoveMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "import sqlite3\n",
    "from langgraph.checkpoint.sqlite import  SqliteSaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01a2cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0090aa18",
   "metadata": {},
   "source": [
    "## Defining the Graph State\n",
    "\n",
    "The state extends `MessagesState`, which already includes\n",
    "a reducer for accumulating messages.\n",
    "\n",
    "An additional `summary` field is added to maintain\n",
    "a compact representation of the conversation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970ca4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(MessagesState):\n",
    "    summary: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a804bb",
   "metadata": {},
   "source": [
    "## Chat Model Initialization\n",
    "\n",
    "A deterministic chat model is used to ensure\n",
    "stable summaries and responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b1c80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-5-nano\", \n",
    "    temperature=0, \n",
    "    model_kwargs= {\"text\":{\"verbosity\": 'low'}},\n",
    "    ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1325ae79",
   "metadata": {},
   "source": [
    "## Node: Ask Question\n",
    "\n",
    "This node prompts the user for a new question\n",
    "and appends it to the message history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7767e627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(state: State) -> State:\n",
    "    \n",
    "    print(f\"\\n-------> ENTERING ask_question:\")\n",
    "    \n",
    "    question = \"What is your question?\"\n",
    "    print(question)\n",
    "    \n",
    "    return State(messages = [AIMessage(question), HumanMessage(input())])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a8a8a4",
   "metadata": {},
   "source": [
    "## Node: Chatbot\n",
    "\n",
    "This node:\n",
    "- Reads the full message history\n",
    "- Injects the running summary as a system message\n",
    "- Generates a context-aware response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224b4123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(state: State) -> State:\n",
    "    \n",
    "    print(f\"\\n-------> ENTERING chatbot:\")\n",
    "    for i in state[\"messages\"]:\n",
    "        i.pretty_print()\n",
    "        \n",
    "    system_message = f'''\n",
    "    Here's a quick summary of what's been discussed so far:\n",
    "    {state.get(\"summary\", \"\")}\n",
    "    \n",
    "    Keep this in mind as you answer the next question.\n",
    "    '''\n",
    "    \n",
    "    response = chat.invoke([SystemMessage(system_message)] + state[\"messages\"])\n",
    "    response.pretty_print()\n",
    "    \n",
    "    return State(messages = [response])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e566078",
   "metadata": {},
   "source": [
    "## Node: Summarize Messages\n",
    "\n",
    "To prevent unbounded context growth, this node:\n",
    "- Summarizes the recent conversation\n",
    "- Updates the running summary\n",
    "- Removes detailed message history using `RemoveMessage`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531880ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_messages(state: State) -> State:\n",
    "    print(f\"\\n-------> ENTERING summarize_messages:\")\n",
    "    \n",
    "    new_conversation = \"\"\n",
    "    for i in state[\"messages\"]:\n",
    "        new_conversation += f\"{i.type}: {i.content}\\n\\n\"\n",
    "        \n",
    "    summary_instructions = f'''\n",
    "Update the ongoing summary by incorporating the new lines of conversation below.  \n",
    "Build upon the previous summary rather than repeating it so that the result  \n",
    "reflects the most recent context and developments.\n",
    "\n",
    "\n",
    "Previous Summary:\n",
    "{state.get(\"summary\", \"\")}\n",
    "\n",
    "New Conversation:\n",
    "{new_conversation}\n",
    "'''\n",
    "    \n",
    "    print(summary_instructions)\n",
    "    \n",
    "    summary = chat.invoke([HumanMessage(summary_instructions)])\n",
    "    \n",
    "    remove_messages = [RemoveMessage(id = i.id) for i in state[\"messages\"][:]]\n",
    "    \n",
    "    return State(messages = remove_messages, summary = summary.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e767aa1",
   "metadata": {},
   "source": [
    "## Defining the Graph\n",
    "\n",
    "The graph follows a simple conversational flow:\n",
    "1. Ask a question\n",
    "2. Generate a response\n",
    "3. Summarize and prune message history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32295e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01583241",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.add_node(\"ask_question\", ask_question)\n",
    "graph.add_node(\"chatbot\", chatbot)\n",
    "graph.add_node(\"summarize_messages\", summarize_messages)\n",
    "\n",
    "graph.add_edge(START, \"ask_question\")\n",
    "graph.add_edge(\"ask_question\", \"chatbot\")\n",
    "graph.add_edge(\"chatbot\", \"summarize_messages\")\n",
    "graph.add_edge(\"summarize_messages\", END)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8203250",
   "metadata": {},
   "source": [
    "## Checkpointing and Persistence\n",
    "\n",
    "Two checkpointing strategies are supported:\n",
    "\n",
    "- `InMemorySaver`  \n",
    "  - State persists only for the current kernel session\n",
    "\n",
    "- `SqliteSaver`  \n",
    "  - State is persisted to disk\n",
    "  - Enables recovery across executions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b53dc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path =  \"./LangGraph_DB/langgraph.db\"\n",
    "con = sqlite3.connect(database=db_path, check_same_thread=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4a86d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpointer =  InMemorySaver()  #for kernal sessions memory persistence\n",
    "checkpointer =  SqliteSaver(con)\n",
    "\n",
    "graph_compiled = graph.compile(checkpointer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3788890b",
   "metadata": {},
   "source": [
    "## Compiling and Executing the Graph\n",
    "\n",
    "Checkpointing is enabled during compilation.\n",
    "Each conversation is scoped using a `thread_id`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6484cabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_compiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977804b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config1 = {\"configurable\" : {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e1b0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_compiled.invoke(State(),config1)   #Instructs the graphs to create in-memory checkpoint and associate them with the specific thread, enabling thread-level persistence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bedb7ac",
   "metadata": {},
   "source": [
    "## Inspecting Checkpointed State History\n",
    "\n",
    "LangGraph allows inspection of all intermediate\n",
    "states stored during execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e34bd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_states = [i for i in graph_compiled.get_state_history(config1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205e3283",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af470d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in graph_states[::-1]:\n",
    "    print(f'''\n",
    "    Messages: {i.values['messages']}\n",
    "    Summary: {i.values.get('summary', '')}\n",
    "    Next: {i.next}\n",
    "    Step: {i.metadata[\"step\"]}\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983c2b6a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated short-term memory in LangGraph by:\n",
    "\n",
    "- Accumulating conversation messages using `MessagesState`\n",
    "- Summarizing and pruning history to manage context size\n",
    "- Using checkpointing for state persistence\n",
    "- Comparing in-memory and SQLite-backed storage\n",
    "\n",
    "This pattern enables scalable, multi-turn conversational\n",
    "systems with controlled memory growth.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langraph_env",
   "language": "python",
   "name": "langraph_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
