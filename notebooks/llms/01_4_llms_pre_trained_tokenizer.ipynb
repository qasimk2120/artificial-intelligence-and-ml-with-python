{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62f06cca",
   "metadata": {},
   "source": [
    "# ðŸ¤— Tokenization with Hugging Face AutoTokenizer\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how text is tokenized using different\n",
    "pre-trained Transformer tokenizers.\n",
    "\n",
    "The examples show how sentences are:\n",
    "- Converted into tokens\n",
    "- Mapped to token IDs\n",
    "- Decoded back into tokens\n",
    "- Interpreted using model-specific special tokens\n",
    "\n",
    "---\n",
    "\n",
    "## What This Code Does\n",
    "\n",
    "1. **Load a Pre-trained Tokenizer (BERT)**  \n",
    "   Loads the `bert-base-uncased` tokenizer using `AutoTokenizer`.\n",
    "\n",
    "2. **Encode a Sentence**  \n",
    "   Converts the input sentence into token IDs and attention-related fields\n",
    "   using the tokenizerâ€™s default encoding method.\n",
    "\n",
    "3. **Tokenize Text**  \n",
    "   Splits the sentence into individual tokens according to the modelâ€™s\n",
    "   tokenization rules.\n",
    "\n",
    "4. **Convert Tokens to Token IDs**  \n",
    "   Maps each token to its corresponding numerical ID from the tokenizerâ€™s\n",
    "   vocabulary.\n",
    "\n",
    "5. **Decode Token IDs**  \n",
    "   Converts token IDs back into readable tokens.\n",
    "\n",
    "6. **Inspect Special Tokens (BERT)**  \n",
    "   Decodes special token IDs such as:\n",
    "   - `[CLS]` â†’ marks the start of a sequence\n",
    "   - `[SEP]` â†’ marks the end or separation of sequences\n",
    "\n",
    "7. **Repeat the Process with a Different Model (XLNet)**  \n",
    "   Loads the `xlnet-base-cased` tokenizer and applies the same steps to show\n",
    "   how tokenization and special tokens differ between models.\n",
    "\n",
    "8. **Inspect Special Tokens (XLNet)**  \n",
    "   Decodes model-specific special tokens used by XLNet to represent\n",
    "   sequence boundaries and structure.\n",
    "\n",
    "---\n",
    "\n",
    "## Output\n",
    "\n",
    "- Tokenized representations of the input sentence\n",
    "- Numerical token IDs used by each model\n",
    "- Decoded tokens and special tokens specific to each tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2eb7442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a57bfb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e69c1445",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dbbff12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence =  \"I'm so excited to learn about Transformers library!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9587e77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(sentence)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c230b6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71983005",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "659b4060",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "print(decoded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccab05c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(101)  # Example: Decoding the token ID for [CLS] #special token added by our tokenizer to indicate the start of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fc607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(102)  # Example: Decoding the token ID for [SEP] #special token added by our tokenizer to indicate the end of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d2a090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#another model\n",
    "model2 = \"xlnet-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2bd19da",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2 = AutoTokenizer.from_pretrained(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfacc24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer2(sentence)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1da1fec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens =  tokenizer2.tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36453db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = tokenizer2.convert_tokens_to_ids(tokens)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6410de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2.decode(4)    #special token for this model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2917159d",
   "metadata": {},
   "source": [
    "### Special Tokens are specific placeholders or markers that help the model perform various tasks, \n",
    " - helps the model understand structure and context\n",
    " - guide model behaviour \n",
    " - ensure output of tokenization is in a format that model comprehends\n",
    " - cls and sep (classification and separator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5c513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2.decode(3)  #another special token for this model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (huggingfaceenv)",
   "language": "python",
   "name": "huggingfaceenv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
