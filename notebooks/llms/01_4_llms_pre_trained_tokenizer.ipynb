{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62f06cca",
   "metadata": {},
   "source": [
    "# ü§ó Hugging Face Tokenization and Model Usage\n",
    "\n",
    "This notebook demonstrates how Hugging Face Transformers handle:\n",
    "- Text tokenization\n",
    "- Model-specific special tokens\n",
    "- Integration with PyTorch\n",
    "- Saving and loading pre-trained models\n",
    "\n",
    "The focus is on understanding how text is prepared for models\n",
    "and how models are executed and reused in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce030232",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Tokenization with AutoTokenizer (BERT)\n",
    "\n",
    "In this section, we use the `bert-base-uncased` tokenizer to observe how\n",
    "a sentence is transformed into tokens and numerical IDs.\n",
    "\n",
    "The steps include:\n",
    "- Encoding text into model inputs\n",
    "- Tokenizing text into subword tokens\n",
    "- Converting tokens to token IDs\n",
    "- Decoding token IDs back to tokens\n",
    "- Inspecting special tokens used by BERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eb7442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a57bfb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e69c1445",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dbbff12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence =  \"I'm so excited to learn about Transformers library!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9587e77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1045, 1005, 1049, 2061, 7568, 2000, 4553, 2055, 19081, 3075, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(sentence)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c230b6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', \"'\", 'm', 'so', 'excited', 'to', 'learn', 'about', 'transformers', 'library', '!']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71983005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1045, 1005, 1049, 2061, 7568, 2000, 4553, 2055, 19081, 3075, 999]\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "659b4060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', \"'\", 'm', 'so', 'excited', 'to', 'learn', 'about', 'transformers', 'library', '!']\n"
     ]
    }
   ],
   "source": [
    "decoded_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "print(decoded_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7776fa66",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Special Tokens in BERT\n",
    "\n",
    "BERT tokenizers automatically add special tokens to represent\n",
    "sentence boundaries and structure.\n",
    "\n",
    "In this section, we explicitly decode:\n",
    "- `[CLS]` ‚Üí marks the start of a sequence\n",
    "- `[SEP]` ‚Üí marks the end or separation of sequences\n",
    "\n",
    "These tokens are required for correct model behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccab05c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(101)  # Example: Decoding the token ID for [CLS] #special token added by our tokenizer to indicate the start of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32fc607d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[SEP]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(102)  # Example: Decoding the token ID for [SEP] #special token added by our tokenizer to indicate the end of a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736cb853",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Tokenization with a Different Model (XLNet)\n",
    "\n",
    "Here, we repeat the same tokenization steps using the\n",
    "`xlnet-base-cased` tokenizer.\n",
    "\n",
    "This demonstrates that:\n",
    "- Tokenization rules differ between models\n",
    "- Token IDs and special tokens are model-specific\n",
    "- Each architecture defines its own input format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d2a090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#another model\n",
    "model2 = \"xlnet-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2bd19da",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2 = AutoTokenizer.from_pretrained(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfacc24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [35, 26, 98, 102, 5564, 22, 1184, 75, 17, 21442, 270, 2992, 136, 4, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer2(sentence)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1da1fec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‚ñÅI', \"'\", 'm', '‚ñÅso', '‚ñÅexcited', '‚ñÅto', '‚ñÅlearn', '‚ñÅabout', '‚ñÅ', 'Transform', 'ers', '‚ñÅlibrary', '!']\n"
     ]
    }
   ],
   "source": [
    "tokens =  tokenizer2.tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36453db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35, 26, 98, 102, 5564, 22, 1184, 75, 17, 21442, 270, 2992, 136]\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer2.convert_tokens_to_ids(tokens)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e511cba",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Special Tokens in XLNet\n",
    "\n",
    "XLNet uses a different set of special tokens compared to BERT.\n",
    "\n",
    "In this section, we decode XLNet-specific token IDs to observe\n",
    "how sequence structure is represented differently across models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6410de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sep>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2.decode(4)    #special token for this model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2917159d",
   "metadata": {},
   "source": [
    "### Special Tokens are specific placeholders or markers that help the model perform various tasks, \n",
    " - helps the model understand structure and context\n",
    " - guide model behaviour \n",
    " - ensure output of tokenization is in a format that model comprehends\n",
    " - cls and sep (classification and separator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad5c513f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<cls>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2.decode(3)  #another special token for this model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed16c5a2",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Using Hugging Face Models with PyTorch\n",
    "\n",
    "In this section, we integrate Hugging Face Transformers with PyTorch.\n",
    "\n",
    "The steps include:\n",
    "- Tokenizing text and returning PyTorch tensors\n",
    "- Loading a fine-tuned sequence classification model\n",
    "- Running inference without gradient computation\n",
    "- Extracting model logits\n",
    "- Mapping predicted class IDs to human-readable labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3ba8fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b41389c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm so excited to learn about Transformers library!\n"
     ]
    }
   ],
   "source": [
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3fdd829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [35, 26, 98, 102, 5564, 22, 1184, 75, 17, 21442, 270, 2992, 136, 4, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "221f6f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")  #finetuned model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59129d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  1049,  2061,  7568,  2000,  4553,  2055, 19081,\n",
      "          3075,   999,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "input_ids_pt = tokenizer(sentence, return_tensors=\"pt\")  #pt for pytorch\n",
    "print(input_ids_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10cde28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "825597c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POSITIVE'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(**input_ids_pt).logits\n",
    "\n",
    "predicted_class_id = logits.argmax().item()\n",
    "model.config.id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8833f6a4",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Saving and Loading Models\n",
    "\n",
    "Finally, we demonstrate how to persist models and tokenizers locally.\n",
    "\n",
    "This includes:\n",
    "- Saving a tokenizer to disk\n",
    "- Saving a fine-tuned model to disk\n",
    "- Reloading both components for future use\n",
    "\n",
    "This is essential for deployment and reuse without retraining.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a03e9f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory= \"F:\\Project_folder\\models_directory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3954a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('F:\\\\Project_folder\\\\models_directory\\\\tokenizer_config.json',\n",
       " 'F:\\\\Project_folder\\\\models_directory\\\\special_tokens_map.json',\n",
       " 'F:\\\\Project_folder\\\\models_directory\\\\vocab.txt',\n",
       " 'F:\\\\Project_folder\\\\models_directory\\\\added_tokens.json',\n",
       " 'F:\\\\Project_folder\\\\models_directory\\\\tokenizer.json')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ecc74e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a61a0fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizer = AutoTokenizer.from_pretrained(model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d43ca1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = AutoModelForSequenceClassification.from_pretrained(model_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d7488d",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- Tokenization is model-specific\n",
    "- Special tokens control sequence structure\n",
    "- Transformers integrate seamlessly with PyTorch\n",
    "- Pre-trained models can be saved and reused efficiently\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (huggingfaceenv)",
   "language": "python",
   "name": "huggingfaceenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
