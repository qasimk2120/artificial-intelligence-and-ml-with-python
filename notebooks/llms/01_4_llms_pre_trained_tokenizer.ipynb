{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62f06cca",
   "metadata": {},
   "source": [
    "# ü§ó Hugging Face Tokenization and Model Usage\n",
    "\n",
    "This notebook demonstrates how Hugging Face Transformers handle:\n",
    "\n",
    "- Text tokenization  \n",
    "- Model-specific special tokens  \n",
    "- Integration with PyTorch  \n",
    "- Saving and loading pre-trained models  \n",
    "\n",
    "The focus is on understanding how text is prepared for models\n",
    "and how models are executed and reused in practice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb7442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eb5db7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Tokenization with AutoTokenizer (BERT)\n",
    "\n",
    "In this section, we use the `bert-base-uncased` tokenizer to observe how\n",
    "a sentence is transformed into tokens and numerical IDs.\n",
    "\n",
    "The steps include:\n",
    "- Encoding text into model inputs  \n",
    "- Tokenizing text into subword tokens  \n",
    "- Converting tokens to token IDs  \n",
    "- Decoding token IDs back to tokens  \n",
    "- Inspecting special tokens used by BERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57bfb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69c1445",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbbff12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence =  \"I'm so excited to learn about Transformers library!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9587e77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(sentence)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c230b6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71983005",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659b4060",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "print(decoded_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7776fa66",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Special Tokens in BERT\n",
    "\n",
    "BERT tokenizers automatically add special tokens to represent\n",
    "sentence boundaries and structure.\n",
    "\n",
    "In this section, we explicitly decode:\n",
    "- `[CLS]` ‚Üí marks the start of a sequence  \n",
    "- `[SEP]` ‚Üí marks the end or separation of sequences  \n",
    "\n",
    "These tokens are required for correct model behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccab05c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(101)  # Example: Decoding the token ID for [CLS] #special token added by our tokenizer to indicate the start of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fc607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(102)  # Example: Decoding the token ID for [SEP] #special token added by our tokenizer to indicate the end of a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736cb853",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Tokenization with a Different Model (XLNet)\n",
    "\n",
    "Here, we repeat the same tokenization steps using the\n",
    "`xlnet-base-cased` tokenizer.\n",
    "\n",
    "This demonstrates that:\n",
    "- Tokenization rules differ between models  \n",
    "- Token IDs and special tokens are model-specific  \n",
    "- Each architecture defines its own input format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2a090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#another model\n",
    "model2 = \"xlnet-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bd19da",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2 = AutoTokenizer.from_pretrained(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfacc24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer2(sentence)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da1fec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens =  tokenizer2.tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36453db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = tokenizer2.convert_tokens_to_ids(tokens)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e511cba",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Special Tokens in XLNet\n",
    "\n",
    "XLNet uses a different set of special tokens compared to BERT.\n",
    "\n",
    "In this section, we decode XLNet-specific token IDs to observe\n",
    "how sequence structure is represented differently across models.\n",
    "\n",
    "### Notes on Special Tokens\n",
    "- Help the model understand structure and context  \n",
    "- Guide model behavior  \n",
    "- Ensure tokenized input matches model expectations  \n",
    "- `[CLS]` and `[SEP]` equivalents differ by architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6410de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2.decode(4)    #special token for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5c513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2.decode(3)  #another special token for this model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed16c5a2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Using Hugging Face Models with PyTorch\n",
    "\n",
    "In this section, we integrate Hugging Face Transformers with PyTorch.\n",
    "\n",
    "The steps include:\n",
    "- Tokenizing text and returning PyTorch tensors  \n",
    "- Loading a fine-tuned sequence classification model  \n",
    "- Running inference without gradient computation  \n",
    "- Extracting model logits  \n",
    "- Mapping predicted class IDs to human-readable labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ba8fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b41389c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fdd829",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221f6f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")  #finetuned model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59129d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_pt = tokenizer(sentence, return_tensors=\"pt\")  #pt for pytorch\n",
    "print(input_ids_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cde28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825597c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(**input_ids_pt).logits\n",
    "\n",
    "predicted_class_id = logits.argmax().item()\n",
    "model.config.id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8833f6a4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ Saving and Loading Models\n",
    "\n",
    "Finally, we demonstrate how to persist models and tokenizers locally.\n",
    "\n",
    "This includes:\n",
    "- Saving a tokenizer to disk  \n",
    "- Saving a fine-tuned model to disk  \n",
    "- Reloading both components for future use  \n",
    "\n",
    "This is essential for deployment and reuse without retraining.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03e9f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory= \"F:\\Project_folder\\models_directory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3954a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc74e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61a0fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizer = AutoTokenizer.from_pretrained(model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43ca1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = AutoModelForSequenceClassification.from_pretrained(model_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d7488d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- Tokenization is model-specific  \n",
    "- Special tokens control sequence structure  \n",
    "- Transformers integrate seamlessly with PyTorch  \n",
    "- Pre-trained models can be saved and reused efficiently\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (huggingfaceenv)",
   "language": "python",
   "name": "huggingfaceenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
