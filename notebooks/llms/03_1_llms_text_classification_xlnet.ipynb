{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "258cbf7e",
   "metadata": {},
   "source": [
    "# ðŸ¤– Text Classification with XLNet\n",
    "\n",
    "This notebook demonstrates fine-tuning **XLNet** for multi-class\n",
    "text classification on an emotion-labeled dataset.\n",
    "\n",
    "The workflow covers:\n",
    "- Text preprocessing and dataset balancing  \n",
    "- Tokenization using an XLNet tokenizer  \n",
    "- Fine-tuning a pre-trained XLNet model  \n",
    "- Model evaluation and inference on unseen text  \n",
    "\n",
    "The implementation highlights practical understanding of\n",
    "Transformer-based sequence classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130b18a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from cleantext import clean\n",
    "import re\n",
    "from transformers import XLNetTokenizer, XLNetForSequenceClassification, TrainingArguments, Trainer, pipeline\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from emoji import replace_emoji\n",
    "import datasets\n",
    "import evaluate\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adc27d6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model Background\n",
    "\n",
    "XLNet is a Transformer-based language model that improves upon\n",
    "earlier architectures by using **permutation-based training**.\n",
    "\n",
    "### Key characteristics\n",
    "- Two variants: **Base** and **Large**  \n",
    "- XLNet Base: ~110M parameters  \n",
    "- XLNet Large: ~340M parameters  \n",
    "- Built on a Transformer-XL architecture  \n",
    "\n",
    "### Architectural comparison\n",
    "- **GPT**: autoregressive (left-to-right prediction)  \n",
    "- **BERT**: masked language modeling (bidirectional context)  \n",
    "- **XLNet**: permutation-based modeling (captures both left and right context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1997ff",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Dataset Loading\n",
    "\n",
    "Emotion-labeled text data is loaded from separate\n",
    "training, validation, and test files.\n",
    "\n",
    "The datasets are combined to enable:\n",
    "- Unified preprocessing  \n",
    "- Class balancing  \n",
    "- Controlled train/validation/test splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89578300",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"../../data/emotions_data/emotion-labels-train.csv\")\n",
    "data_test = pd.read_csv(\"../../data/emotions_data/emotion-labels-test.csv\")\n",
    "data_val = pd.read_csv(\"../../data/emotions_data/emotion-labels-val.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099baadf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Text Preprocessing and Class Balancing\n",
    "\n",
    "The raw text is cleaned to reduce noise and improve model performance.\n",
    "\n",
    "### Preprocessing steps\n",
    "- Emoji removal  \n",
    "- Removal of user mentions  \n",
    "- Basic text normalization  \n",
    "\n",
    "### Class balancing\n",
    "- Group samples by label  \n",
    "- Downsample each class to the minimum class size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05509a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3d4251",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  pd.concat([data_train, data_test, data_val], ignore_index=True)  #combine all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fba3533",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_clean'] = data['text'].apply(lambda x: replace_emoji(x, replace=''))  #remove emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464eb497",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_clean'].head()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a0d5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_clean'] = data['text_clean'].apply(lambda x: re.sub('@[^\\s]+', '', x))  # Removes Punctuations + mentions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c5be95",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_clean'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881349fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'].value_counts().plot(kind='bar')   #visualize label distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162545e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "g =  data.groupby('label')  #group by label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ef70a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(g.apply(lambda x: x.sample(g.size().min()).reset_index(drop=True)))  #balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3937df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27150335",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Label Encoding and Dataset Splits\n",
    "\n",
    "Emotion labels are encoded into integer values for model compatibility.\n",
    "\n",
    "The dataset is split into:\n",
    "- Training set  \n",
    "- Validation set  \n",
    "- Test set  \n",
    "\n",
    "This separation enables reliable evaluation during fine-tuning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca58dcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label_int'] = LabelEncoder().fit_transform(data['label'])  #encode labels to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb04a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LABEL = 4  #number of unique labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf66e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split, test_split = train_test_split(data, train_size=0.8)    #split data into train and test\n",
    "train_split, val_split = train_test_split(train_split, test_size=0.9)  #split train data into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5d4541",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_split))\n",
    "print(len(test_split))\n",
    "print(len(val_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef28ebab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Dataset Preparation for Training\n",
    "\n",
    "The processed data is converted into Hugging Face `Dataset`\n",
    "objects for efficient batching and integration with\n",
    "the Transformers training pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e5f808",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df =pd.DataFrame({\n",
    "    \"label\" : train_split.label_int.values,\n",
    "    \"text\" : train_split.text_clean.values\n",
    "})    #train dataframe  \n",
    "test_df =pd.DataFrame({\n",
    "    \"label\" : test_split.label_int.values,\n",
    "    \"text\" : test_split.text_clean.values\n",
    "})    #test dataframe  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6c79d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = datasets.Dataset.from_dict(train_df)\n",
    "test_df = datasets.Dataset.from_dict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2bff64",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = datasets.DatasetDict({\"train\": train_df, \"test\": test_df})\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfd3299",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Tokenization with XLNet\n",
    "\n",
    "Text samples are tokenized using the XLNet tokenizer.\n",
    "\n",
    "Tokenization includes:\n",
    "- Padding to a fixed maximum length  \n",
    "- Truncation of long sequences  \n",
    "- Generation of input IDs, token type IDs, and attention masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce95e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')  #load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc02c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):  #tokenization function\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True,  max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02229278",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = dataset_dict.map(tokenize_function, batched=True)  #tokenize the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fc0161",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a72ebe",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Fine-Tuning XLNet for Classification\n",
    "\n",
    "A pre-trained XLNet model is loaded with a sequence\n",
    "classification head.\n",
    "\n",
    "To keep training efficient:\n",
    "- A small subset of the dataset is used  \n",
    "- Accuracy is used as the evaluation metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3def9841",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_datasets[\"train\"]['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc519aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_datasets[\"train\"]['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ece9c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(5)  #decode input id to token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc594538",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_datasets[\"train\"]['token_type_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dda2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_datasets[\"train\"][\"attention_mask\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7434f003",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(100))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b00220",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=NUM_LABEL, id2label = {0: 'anger', 1: 'fear', 2: 'joy', 3: 'sadness'})  #load pre-trained model with classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4554af25",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9de79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2b70c8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model Training and Evaluation\n",
    "\n",
    "The model is trained using the Hugging Face `Trainer` API.\n",
    "\n",
    "Evaluation is performed at each epoch to monitor:\n",
    "- Classification accuracy  \n",
    "- Training convergence behavior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42fc624",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./test_trainer\", eval_strategy=\"epoch\",  #evaluation strategy at each epoch\n",
    "    num_train_epochs=3,)  #number of training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23239e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         #the pre-trained model\n",
    "    args=training_args,                  #training arguments\n",
    "    train_dataset=small_train_dataset,   #training dataset\n",
    "    eval_dataset=small_eval_dataset,      #evaluation dataset\n",
    "    compute_metrics=compute_metrics,      #evaluation metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045c8a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b6c4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d848bd1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model Saving and Inference\n",
    "\n",
    "After training:\n",
    "- The fine-tuned model is saved to disk  \n",
    "- Reloaded for inference  \n",
    "- Used with a Transformers pipeline to predict\n",
    "  emotion labels on unseen text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f76c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151d7136",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = XLNetForSequenceClassification.from_pretrained(\"fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ec9b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf =  pipeline(\"text-classification\", model=fine_tuned_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1b0856",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_int = random.randint(0, len(val_split))\n",
    "print(val_split['text_clean'][rand_int])\n",
    "answer = clf(val_split['text_clean'][rand_int], top_k=None)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d354a28",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Practical fine-tuning of XLNet for text classification  \n",
    "- Handling class imbalance in real-world datasets  \n",
    "- End-to-end training using the Transformers ecosystem  \n",
    "- Deployment-ready inference using pipelines  \n",
    "\n",
    "The work reflects applied knowledge of modern\n",
    "Transformer architectures and training workflows.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (huggingfaceenv)",
   "language": "python",
   "name": "huggingfaceenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
