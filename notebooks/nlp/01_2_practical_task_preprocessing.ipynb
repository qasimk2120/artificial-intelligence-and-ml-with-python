{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f3e1f0f-3c39-463b-9e55-81f7f678ed86",
   "metadata": {},
   "source": [
    "# 02 Preprocessing Practical Example on Realistic Text Data (NLP)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates a complete text preprocessing pipeline example using **NLTK**, **Pandas**, and **regular expressions** on real-world hotel review data.  \n",
    "Each preprocessing step is applied incrementally, with results stored in new columns to preserve intermediate transformations.\n",
    "\n",
    "---\n",
    "\n",
    "## Libraries Used\n",
    "\n",
    "- **NLTK**\n",
    "  - Tokenization (`word_tokenize`)\n",
    "  - Stopwords\n",
    "  - Stemming (`PorterStemmer`)\n",
    "  - Lemmatization (`WordNetLemmatizer`)\n",
    "  - N-grams\n",
    "- **Pandas** for data handling\n",
    "- **re** for regular expression–based text cleaning\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Loading and Inspection\n",
    "\n",
    "- Load hotel reviews from a CSV file\n",
    "- Inspect dataset structure using:\n",
    "  - `data.info()`\n",
    "  - `data.head()`\n",
    "- Access individual review entries for inspection\n",
    "\n",
    "---\n",
    "\n",
    "## Text Preprocessing Steps\n",
    "\n",
    "### 1. Lowercasing Text\n",
    "- Convert all review text to lowercase\n",
    "- Store results in a new column to preserve original data\n",
    "- Use Pandas `.str` string accessor to apply changes to all rows\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Stopword Removal\n",
    "- Load English stopwords from NLTK\n",
    "- Explicitly **retain the word “not”** to preserve sentiment meaning\n",
    "- Remove stopwords using a custom `apply()` function\n",
    "- Store cleaned text in a new column\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Removing Punctuation and Special Characters\n",
    "- Replace asterisk (`*`) symbols with the word `\"star\"`\n",
    "- Remove all remaining punctuation using regular expressions\n",
    "- Apply transformations row-by-row using `axis=1`\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Tokenization\n",
    "- Convert cleaned review text into tokens\n",
    "- Store tokens as lists in a new column\n",
    "- Each review becomes a list of individual words\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Stemming\n",
    "- Apply **Porter Stemmer** to reduce words to their root forms\n",
    "- Store stemmed tokens in a separate column\n",
    "- Allows comparison between original, tokenized, and stemmed text\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Lemmatization\n",
    "- Apply **WordNet Lemmatizer** to normalize words to their dictionary form\n",
    "- Store lemmatized tokens separately\n",
    "- Enables direct comparison between stemming and lemmatization results\n",
    "\n",
    "---\n",
    "\n",
    "## Preparing Tokens for N-gram Analysis\n",
    "\n",
    "- Combine all token lists from every review into **one single list**\n",
    "- Use Python’s `sum()` function to flatten the list of lists\n",
    "- This produces a corpus-wide token list suitable for frequency analysis\n",
    "\n",
    "---\n",
    "\n",
    "## N-gram Analysis\n",
    "\n",
    "- Generate:\n",
    "  - **Unigrams (1-grams)**\n",
    "  - **Bigrams (2-grams)**\n",
    "  - **Trigrams (3-grams)**\n",
    "- Count occurrences using:\n",
    "  - `nltk.ngrams`\n",
    "  - `pd.Series().value_counts()`\n",
    "- Display frequency distributions for analysis\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- Each preprocessing step is isolated and reversible\n",
    "- New columns preserve preprocessing history\n",
    "- Stopword customization matters for sentiment analysis\n",
    "- Token preparation is required before n-gram analysis\n",
    "- Stemming and lemmatization serve different purposes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eeb05b0-842a-4136-9154-4e82952966b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4eac5f4-e64d-4f4d-977c-6b3e008fadc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../../data/tripadvisor_hotel_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d48ba9-ffc7-40d7-a703-1c31d4d7b2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()   #to  see what we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf569e5a-44a3-4846-bab6-d71cbecf75d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()  #to take a look t our data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc9d681-ffb2-4139-9572-7300ca6e7af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Review'][0] #specified that we are intrested in row zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb035560-ecc1-4437-afcd-18a059dc6f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 , lower case text on the Review Column\n",
    "#a pandas Series is not a single string — it's a column containing many strings\n",
    "#.str is the string accessor that tells pandas (Apply this operation to every element in this column)\n",
    "data['review_lowercase'] = data['Review'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c41fe1e-39be-46d2-a9ee-badab6d9af4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d264299-0a23-4a1a-a691-cdd527019500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2 , remove stop words from the reviews\n",
    "en_stopwords = stopwords.words('english') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09be3ea8-a35b-4f82-9b7a-8f989188ed0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make sure \"not\" is not within the stopwords we will use .remove to remove from list of stopwords\n",
    "en_stopwords.remove(\"not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bdead4-b6a0-47da-8c8d-8ef54d2798f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's create a new column called Review no stopwords\n",
    "#apply function is very usefull because it lets us take one column and perform a custom operation on every value in it\n",
    "# .split splts each review x into individual words \n",
    "# when preprocessing the text, it is always worth making a new column for each of the steps in your preprocessing\n",
    "data['review-stop-no-stopwords'] = data['review_lowercase'].apply(lambda x: ' '.join([word for word in x.split() if word not in (en_stopwords)] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135768bd-842f-49f0-8425-29415bff6e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['review-stop-no-stopwords'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65f7fa4-8735-4433-969a-b6b064dd23d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3 removing punctuations\n",
    "#first creating a new column\n",
    "#axis = 1 tells python to go row by row rather then column by column\n",
    "data['review-stop-no-stopwords-no-punct'] = data.apply(lambda x: re.sub(r\"[*]\", \"star\", x['review-stop-no-stopwords']), axis=1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600a1dac-59ee-425d-9a55-95db1d04ba81",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()  #here we can see that anywhere on review-stop-no-stopwords column where the text had a aestrik sign, its replaced with star word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bd9673-6836-4ff5-9622-7417900a2975",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['review-stop-no-stopwords-no-punct'] = data.apply(lambda x: re.sub( r\"([^\\w\\s])\", \"\", x['review-stop-no-stopwords-no-punct']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a206a6-8624-4012-8784-eaa67f792a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836b4f0f-1704-46d1-b6fd-bd49259c5482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4 tokenizing the text\n",
    "data['tokenized'] = data.apply(lambda x: word_tokenize(x['review-stop-no-stopwords-no-punct']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bba7b8b-149a-42f1-8fef-94cdf783f40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokenized'] [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eafe96-3c5d-4c4c-bec1-d148c05bb0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 5 stemming the text\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e760875-83d4-4f81-b4a9-0de588679b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['stemmed'] = data['tokenized'].apply(lambda tokens: [ps.stem(token) for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a4f5d1-982b-4225-947e-19854bb062ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head() #confirming if the words have been stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203ca16f-436c-4163-bbe6-ea31443264ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c291828-4242-4523-8a41-ac62110a8d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparison between stemmer and lemitizer\n",
    "data['lemmatized'] = data['tokenized'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7011b3d9-dbc1-4e60-aa0e-85d033234b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lemmatized'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1e2120-30e0-4172-8a93-a743dcc94d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have both stemmed and lemmatized columns, before running N-grams we need to prepare our text in the right format\n",
    "#right now each row in the lemmatized column contains a seperate list of tokens, Each review is stored as its own list of lemmatized words\n",
    "#We need to combine these smaller lists of tokens into one long list that contains every token form all reviews\n",
    "#we can do that using the sum function\n",
    "tokens_clean =  sum(data['lemmatized'], [])   #in python sum() with lists joins them together instead of returning a sum(number)\n",
    "#sum() keeps adding each review's list of token to the empty list as it goes through all indiviual reviews as a result we end up with 1 big list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d835393b-3f86-4249-8882-128f2fae6241",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad6797b-0f5a-4a0e-a119-88cc87dc3139",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = (pd.Series(nltk.ngrams(tokens_clean, 1)).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f269bf4-5ad8-45cd-9bf2-9f94ff0b1cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa7a7c2-e06a-42e1-b0f5-4d24e9138b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = (pd.Series(nltk.ngrams(tokens_clean, 2)).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed443694-7a4c-4ada-ad04-a576b2fc161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75537ba3-c9ed-4d6f-982b-4f8961d7ce32",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = (pd.Series(nltk.ngrams(tokens_clean, 3)).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7911cf73-6b28-470c-bcf2-61ee49a17c4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(trigrams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
