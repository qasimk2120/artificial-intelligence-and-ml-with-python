{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9760ef44-226f-4b24-b995-b2ee73ad81ee",
   "metadata": {},
   "source": [
    "# NLP Text Preprocessing (Udemy AI Engineer Course)\n",
    "This notebook demonstrates Data Clean up Steps and TEXT PREPROCESSING:\n",
    "- Lowercasing\n",
    "- Removing stopwords\n",
    "- Regex preprocessing\n",
    "- Tokenization\n",
    "- Lemmatization / stemming\n",
    "- N-grams\n",
    "Part of the AI & Robotics portfolio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ef1634-b46a-41c5-92ad-c301b015e035",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "1) **Using .lower() function to turn sentences to lower case** \n",
    "Lower method is beneficial in npl because it simplifies the workflow and maintains consistency\n",
    "one benefit of lowercasing words like “Apple” and “apple” is that It allows the model to treat them as the same token, simplifying analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8bcfa8-cb9b-4aab-82d4-c113e235e2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Her cat's name is Luna\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51326017-f371-4fda-ae83-c3e182e6c492",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_sentence = sentence.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfcf344-e67d-4a84-9be2-37feacbd0f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lower_sentence )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8d1edb-139f-43c2-a18b-248e2d7019b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = [\"Could you pass me the TV remote?\", \"It is impossible to find this hotel\", \"Want to go for dinner on Tuesday\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b00cce1-364f-4402-8522-dccdd6bff86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_sentence_list = [x.lower() for x in sentence_list]\n",
    "lower_sentence_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dcc278-5d04-46a1-81d1-31a89056d574",
   "metadata": {},
   "source": [
    "2) **Stop Words removal**\n",
    "we remove stop words e.g. and , of , a and to since they dont have much meaning and simplify the text for smaller and cleaner dataset easier to process\n",
    "for this purpose we use NLTK library, comes with predefined list of stopwords, tools for tokenizng text and datasets for experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52670c32-8ae9-4eb7-84db-32b1105dd907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')           # run once (or guard it)\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fd448f-50f5-407b-80d3-eb8aa1e39abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords = stopwords.words('english')\n",
    "print(en_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d169f48d-f4ec-41ce-8373-a1e93c0e3145",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"it was too far to go to the shop and he did not want her to walk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9ee8a1-6821-4932-bfac-54e2997c1f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_no_stopwords = ' '. join([word for word in sentence.split() if word not in en_stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4b8bd6-b0a6-42f1-acd5-78f8e6a0212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentence_no_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae458bc-bd94-434f-ac63-ffb33c6c2ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords.remove(\"did\")\n",
    "en_stopwords.remove(\"not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f99088-0eeb-4a45-9a44-03a8ff3d9730",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords.append(\"go\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573a2e5a-5819-445c-8b87-b124ca0361c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_no_stopwords_custom =' '.join([word for word in sentence.split() if word not in en_stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6188b16d-760b-47d8-acdc-2c730363207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentence_no_stopwords_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34899354-4309-4b21-bf6b-3ecb6cb5c614",
   "metadata": {},
   "source": [
    "3) **Regular Expressions** \n",
    "Regular Expressions (regex): a special way of writing patterns to search through text \n",
    "e.g. any thre numbers in a row. it can be used to find text, filter or check text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f8d427-2e88-4584-93a4-6b12273220cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Raw Strings e.g. we do \\n which means new line but what if we want it to be \\n in that case we use raw string as r\"\\n\" \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd325f8f-0e25-4228-9761-c35789a1dc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_folder = \"C:\\desktop\\notes\" if we do this we loose n after desktop and \"otes\" also shifted to next line\n",
    "#instead we use raw string \n",
    "my_folder = r\"C:\\desktop\\notes\"\n",
    "print(my_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fdfbd1-b04f-4f62-b3a0-8f15fc38d7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#regex search function\n",
    "result_search  =  re.search(\"pattern\", r\"string to contain the pattern\") #if matches returns matched object, else returns None\n",
    "result_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c376bd-c779-4d15-93c4-051bb95376ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_search_2 =  re.search(\"pattern\", r\"the phrase to find isn't in the string\")\n",
    "print(result_search_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968562f7-4bbe-4a97-88d8-c26cd72e3785",
   "metadata": {},
   "outputs": [],
   "source": [
    "#re.sub searches for a specific text withing a string and replaces it with new content, required 3 arguments\n",
    "#re.sub(\"old text\", \"new text\",  \"the string\"), \n",
    "string = r\"sara was able to help me find the new items i needed quickly\"\n",
    "new_string =  re.sub(\"sara\", \"sarah\", string)\n",
    "new_string   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519a57eb-90c5-4cb6-8b3f-636362d20639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c820bf0-05de-4a76-b44a-a361b2333803",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here if we do \"sarah?\" it means \"h\" is optonal\n",
    "customer_reviews = [\"sam was a great help to me in the store\", \"the cashier was very rude to me, I think her name was elanor\",\n",
    "                    \"amazing work from sadeen!\", \"sarah was able to help me find the items i needed quickly\", \"lucy is such a great addition to the team\",\n",
    "                    \"great service from sara she found me what i wanted\" ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6470598e-b441-4c10-8c4d-ec5da250dfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sarahs_reviews = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e073014e-f96a-4ae5-9381-ecfadee466d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_to_find = r\"sarah?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9878f155-8c38-4225-bb73-2418f628ce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for string in customer_reviews:\n",
    "    if (re.search(pattern_to_find, string)):\n",
    "        sarahs_reviews.append(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447e8798-fd9c-4b0c-9632-f03275759972",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sarahs_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf375c6-1c1f-415c-82c0-76a95052004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ^pattern matches any string that begins with that pattern e.g. r\"^a\" matches any string that beings with 'a'\n",
    "a_reviews = []\n",
    "pattern_to_find = r\"^a\"\n",
    "for string in customer_reviews:\n",
    "    if (re.search(pattern_to_find, string)):\n",
    "        a_reviews.append(string)\n",
    "print(a_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4860cb6d-8f50-4d28-ae42-bc9c975a0a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# $pattern matches any string that ends with that pattern e.g. r\"y$\" matches any string that ends with 'y'\n",
    "y_reviews = []\n",
    "pattern_to_find = r\"y$\"\n",
    "for string in customer_reviews:\n",
    "    if (re.search(pattern_to_find, string)):\n",
    "        y_reviews.append(string)\n",
    "print(y_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f63fdb0-e188-4c93-a221-b490fa57a4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | pipe symbol works like an \"or\" , e.g. checking multiple patterns, here parenthesis are neccassary as they group the words together\n",
    "needwant_reviews = []\n",
    "pattern_to_find = r\"(need|want)ed\"\n",
    "for string in customer_reviews:\n",
    "    if (re.search(pattern_to_find, string)):\n",
    "        needwant_reviews.append(string)\n",
    "print(needwant_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d50d84-b4ac-437f-b9fc-f1bca535cca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing punctations from text using regex, crucial in NLP , bcoz punctuation(:,?\";) gets in the way when we want to analyze words\n",
    "no_punct_reviews = []\n",
    "pattern_to_find = r\"[^\\w\\s]\"   #here square brackets define set of characters we want to match  #inside square brackets ^ means not/negate\n",
    "# \\w stands for word characters \n",
    " #\\s stands for white spaces, \n",
    "#in other words pattern means find anything that is not \n",
    "                                # a word character and not a space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae8b22e-c561-4649-8e7e-886b093edb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "for string in customer_reviews:\n",
    "    no_punct_string = re.sub(pattern_to_find, \"\", string)  #re.sub(old text, new text, string)\n",
    "    no_punct_reviews.append(no_punct_string)\n",
    "print(no_punct_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5632465b-a34c-484a-8094-1f71882d1622",
   "metadata": {},
   "source": [
    "4) **Tokenization**\n",
    "# Critical Step in text preprocessing, which is breaking text into smaller units\n",
    "# common type is word tokenization where is word in a sentence becomes a token\n",
    "# depending upon use cases they can be words, sentences, subwords or characters\n",
    "# smaller text makes it easier to analyze and understand in other words easier for analysis\n",
    "# for example looking at individual words can reveal patterns e.g. which word is most common, how often specific words appear together\n",
    "# we use nltk package here and download punkt_tab a table to figure out where sentences beging and end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498a96a4-882e-4e7a-8b6b-e85aeab27f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beda9fdc-6e63-4aab-b295-9e7f2a239399",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = \"Her cat's name is Luna. Her dog's name is Max\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a1d17a-f965-47b4-886b-2311cf16c0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing sentence tokenization here \n",
    "sent_tokenize(sentences) #splits into individual sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dc4250-3d30-4efa-ba15-a5e00f8afd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing word tokenization here\n",
    "sentence = \"Her cat's name is Luna.\"  \n",
    "word_tokenize(sentence)  #breaks into individual tokens, each word becomes its own token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0463d7-8b8d-48c3-89e6-e49cba9406ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing word tokenization on longer text\n",
    "sentence = \"Her cat's name is Luna and her dog's name is Max\"\n",
    "word_tokenize(sentence)  #breaks into individual tokens, each word becomes its own token\n",
    "#if we had turned to lower case/preprocessed before tokenization, the tokens would not have treated word \"her\" as seperate tokens\n",
    "#by lowering case we could have ensured consistency "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932881e4-b0a4-4750-a5cb-8b35fb89a8de",
   "metadata": {},
   "source": [
    "5) **Stemming**\n",
    "# Standardization of text, means makng the text more uniform so that different forms of the same words do not confuse the analysis\n",
    "# common way of standardization is stemming,where words are reduced to base form e.g connecting or connected will be reduced to base form connnect\n",
    "# stemming works by chopping off endings or suffixes\n",
    "# downsize is result isn't a proper word or doesn't look meaningfull e.g. studies might be stemmed to something like stud\n",
    "# we standardize because it reduces the number of unique words \n",
    "# reduces noise in the data set making data cleaner and simpler an essential step in Machine Learning\n",
    "# we use PorterStemmer from the nltk.stem module which reduces english words to simpler bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadb697b-dcb6-4e60-8ed8-067e84008bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e67673-61f3-4029-9240-077bcfd9bbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer() #we create a PorterStemmer Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0cd4de-1929-4974-adab-e3f0b0961f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example 1\n",
    "connect_tokens = ['connecting', 'connected', 'connectivity','connect', 'connects']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757f62fe-1b2c-45fc-8456-c350191be47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in connect_tokens:\n",
    "    print(t, \": \" , ps.stem(t))   #using .stem() method to stem tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93508ae6-6dd8-48fe-b59b-1cae42923b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example 2\n",
    "learn_tokens = ['learned', 'learning', 'learn' , 'learner', 'learners']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6ddaa7-3f15-441c-84da-c50682b9abc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in learn_tokens:\n",
    "    print(t, \": \" , ps.stem(t)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbff08fb-c6dd-46cc-bb20-b9875cdf773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example 3\n",
    "like_tokens = ['likes', 'better', 'worse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264a575d-0f4a-4b26-ac3f-fac0c21cb0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in like_tokens:\n",
    "    print(t, \": \" , ps.stem(t)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c898c340-f205-44b9-90ca-a47dcc84512b",
   "metadata": {},
   "source": [
    "6) **lemmatization**\n",
    "# lemmatization reduces a word to a meaning base form while preserving its intended meaning\n",
    "# More sophiscated as it references a predefined dictionary to find correct base, it uses that predefined dictionary to keep the context of the word\n",
    "# We usually end up with real, meaningfull words\n",
    "# Trade off is that we may still have more unique words compared to stemming\n",
    "# we are going to use NLTK library and we are going to download wordnet as its an extensive database of English, A dictionary Lemmatizer uses to make    sure the base forms it produces are real words\n",
    "# then we import the lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a77ea0d-af1a-4e7b-83d3-3deafab10db6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2243c5af-47e6-4f3c-93dc-597d4eb89350",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() #(lemmatizer Object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e43ed11-ab54-4311-b08c-f7a215edee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in connect_tokens:\n",
    "    print(t, ':', lemmatizer.lemmatize(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2886bacb-cd6e-4135-9115-c0c4b892c7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in learn_tokens:\n",
    "    print(t, ':', lemmatizer.lemmatize(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d765af7-c4ab-4c77-bee4-d501a98589e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in like_tokens:\n",
    "    print(t, ':', lemmatizer.lemmatize(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce38f25-8294-4cbe-9d73-9b72e3f29806",
   "metadata": {},
   "source": [
    "7) **N-grams**\n",
    "#  Help us analyze the relationship between neighboring words\n",
    "#  N-grams basically means a seqeunce of N tokens, value of N tells us how many words are grouped \n",
    "#  when n equals one, we have single words called unigrams N = 1: unigrams e.g. 'I', 'love', 'nlp', examining unigrams helps us understand the basic      building blocks of our text , often the first step in exploring data\n",
    "#  when n equals two, we have pairs of consecuting words called bigrams N = 2: bigrams e.g.  'I love' 'love nlp' \n",
    "#  when n equals three, we have sequences of words called trigrams N = 3: trigrams e.g. 'I love nlp' \n",
    "#  IF N > 3 , We use the word N-grams to describe them\n",
    "#  we are going to use nltk, pandas and matplotlib    \n",
    "#  pandas is the python library for working with structured data fast and intuitive\n",
    "#  core feature in pandas is the data frame which works like a spreadsheet, data frame stores data in rows and columns , making it easier to sort,        filter and group, Perfect for preparing text for analysis, keeping it organized and connecting with other steps in ML workflows\n",
    "#  matplotlib is the python library for creating charts and visualizations\n",
    "#  it can take results of our text analysis and turn them into graphs, easier to spot trends and patterns in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea57ca1-c72f-43a0-a44e-5d459d59536e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e2af03-e24d-4f3b-a599-c9c0e2af17a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [\n",
    "    'the', 'rise', 'of', 'artificial', 'intelligence', 'has', 'led', 'to',\n",
    "    'significant', 'advancements', 'in', 'natural', 'language', 'processing',\n",
    "    'computer', 'vision', 'and', 'other', 'fields', 'machine', 'learning',\n",
    "    'algorithms', 'are', 'becoming', 'more', 'sophisticated', 'enabling',\n",
    "    'computers', 'to', 'perform', 'complex', 'tasks', 'that', 'were', 'once',\n",
    "    'thought', 'to', 'be', 'the', 'exclusive', 'domain', 'of', 'humans',\n",
    "    'with', 'the', 'advent', 'of', 'deep', 'learning', 'neural', 'networks',\n",
    "    'have', 'become', 'even', 'more', 'powerful', 'capable', 'of',\n",
    "    'processing', 'vast', 'amounts', 'of', 'data', 'and', 'learning', 'from',\n",
    "    'it', 'in', 'ways', 'that', 'were', 'not', 'possible', 'before', 'as',\n",
    "    'a', 'result', 'ai', 'is', 'increasingly', 'being', 'used', 'in', 'a',\n",
    "    'wide', 'range', 'of', 'industries', 'from', 'healthcare', 'to',\n",
    "    'finance', 'to', 'transportation', 'and', 'its', 'impact', 'is', 'only',\n",
    "    'set', 'to', 'grow', 'in', 'the', 'years', 'to', 'come'\n",
    "]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74008cb-ed97-4f46-9862-37d48dabe1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams =  (pd.Series(nltk.ngrams(tokens, 1)).value_counts())   #pd.Series is a single column od data in a spreadsheet\n",
    "print('All Unigrams','\\n' , unigrams  )\n",
    "print('Top 10 Unigrams','\\n', unigrams[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3936e792-8c55-4f42-9f26-dc48157aaac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking top 10 unigrams , sorting them so they are in order then .plot.barh with an h in the end which creates a horizontal bar chat, followed \n",
    "#by customization e.g. color=\"lightsalmon\"  width adjustment e.g.  width = 0.9 and define figure size e.g. figsize = (12,8)\n",
    "unigrams[:10].sort_values().plot.barh(color=\"lightsalmon\", width = 0.9, figsize=(12,8))\n",
    "#finally we add tile\n",
    "plt.title(\"10 Most frequently Occuring Unigrams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165e8a11-ce3a-47a8-b0a9-3b7d99c0fc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams =  (pd.Series(nltk.ngrams(tokens, 2)).value_counts())   #pd.Series is a single column od data in a spreadsheet\n",
    "print('All Bigrams','\\n' , bigrams  )\n",
    "print('Top 10 Bigrams','\\n', bigrams[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8b8910-4bf3-4bc9-9cf2-49b7d6481c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams =  (pd.Series(nltk.ngrams(tokens, 3)).value_counts())   #pd.Series is a single column od data in a spreadsheet\n",
    "print('All trigrams','\\n' , trigrams  )\n",
    "print('Top 10 trigrams','\\n', trigrams[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fd7a79-8cc5-4c4a-ab55-56abe492cb4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course_env",
   "language": "python",
   "name": "nlp_course_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
