{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c57d503c",
   "metadata": {},
   "source": [
    "# POS Tagging with spaCy (Personal Practice Notes)\n",
    "\n",
    "This notebook contains my **own breakdown and understanding** of Part-of-Speech (POS) tagging using spaCy and pandas.\n",
    "\n",
    "The goal is not just to make the code work, but to understand:\n",
    "- what spaCy returns,\n",
    "- how pandas is used to organize the data,\n",
    "- and why certain approaches are better than others.\n",
    "\n",
    "These notes are meant for **self-learning and helping fellow learners**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd482e10",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Introduction: Text Tagging in NLP\n",
    "\n",
    "After pre-processing text (for example using n-grams), we can extend our analysis by **tagging** the text.\n",
    "\n",
    "There are two common types of tagging:\n",
    "\n",
    "### 1) Part-of-Speech (POS) Tagging\n",
    "- Labels each word with its grammatical role\n",
    "- Examples: NOUN, VERB, ADJ, DET, PRON\n",
    "\n",
    "### 2) Named Entity Recognition (NER)\n",
    "- Identifies real-world entities such as:\n",
    "  - people\n",
    "  - locations\n",
    "  - organizations\n",
    "  - works of art\n",
    "\n",
    "Tagging helps us:\n",
    "- understand what‚Äôs inside a text\n",
    "- explore language patterns\n",
    "- create features for machine learning models\n",
    "- perform standalone linguistic analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f533bacf",
   "metadata": {},
   "source": [
    "2Ô∏è‚É£ Libraries and Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f416f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b1d231",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Libraries and Model Setup\n",
    "\n",
    "We use:\n",
    "- **spaCy** for NLP processing (tokenization, POS tagging, etc.)\n",
    "- **pandas** for organizing and analyzing structured data\n",
    "\n",
    "The `en_core_web_sm` model is:\n",
    "- lightweight\n",
    "- fast\n",
    "- suitable for learning and small projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe4cb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "emma_ja = \"\"\"emma woodhouse handsome clever and rich with a comfortable home and happy disposition seemed to unite some of the best blessings of existence and had lived nearly twentyone years in the world with very little to distress or vex her she was the youngest of the two daughters of a most affectionate indulgent father and had in consequence of her sisters marriage been mistress of his house from a very early period her mother had died too long ago for her to have more than an indistinct remembrance of her caresses and her place had been supplied by an excellent woman as governess who had fallen little short of a mother in affection sixteen years had miss taylor been in mr woodhouses family less as a governess than a friend very fond of both daughters but particularly of emma between them it was more the intimacy of sisters even before miss taylor had ceased to hold the nominal office of governess the mildness of her temper had hardly allowed her to impose any restraint and the shadow of authority being now long passed away they had been living together as friend and friend very mutually attached and emma doing just what she liked highly esteeming miss taylors judgment but directed chiefly by her own\"\"\"\n",
    "print(emma_ja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2ba629",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_doc = nlp(emma_ja)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ad4bff",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Creating a spaCy Doc\n",
    "\n",
    "A **spaCy Doc** is a **structured object**, not just a string.\n",
    "\n",
    "It contains:\n",
    "- the original text\n",
    "- tokens (words)\n",
    "- linguistic annotations such as POS tags\n",
    "\n",
    "Each token already has:\n",
    "- `token.text` ‚Üí the word\n",
    "- `token.pos_` ‚Üí its part-of-speech tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cef499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Full Text ===\")\n",
    "print(spacy_doc.text)\n",
    "\n",
    "print(spacy_doc[0].text)   # first token\n",
    "print(spacy_doc[0].pos_)   # POS tag of first token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1a98d8",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Extracting Tokens and POS Tags into a DataFrame\n",
    "\n",
    "We want a table with:\n",
    "- one row per word\n",
    "- the word itself\n",
    "- its POS tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f05a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Row-by-row DataFrame concatenation (Instructor‚Äôs approach)\n",
    "pos_df = pd.DataFrame(columns=[\"token\", \"pos_tag\"])\n",
    "\n",
    "for token in spacy_doc:\n",
    "    pos_df = pd.concat(\n",
    "        [\n",
    "            pos_df,\n",
    "            pd.DataFrame.from_records([\n",
    "                {\"token\": token.text, \"pos_tag\": token.pos_}\n",
    "            ])\n",
    "        ],\n",
    "        ignore_index=True\n",
    "    )\n",
    "print(pos_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09bd51f",
   "metadata": {},
   "source": [
    "### ‚ùå Method 1 Notes\n",
    "\n",
    "This approach works, but it is inefficient because:\n",
    "- a new DataFrame is created on every loop iteration\n",
    "- existing data is repeatedly copied\n",
    "- performance degrades for large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164da994",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "\n",
    "for token in spacy_doc:\n",
    "    records.append((token.text, token.pos_))\n",
    "\n",
    "pos_df = pd.DataFrame(records, columns=[\"token\", \"pos_tag\"])\n",
    "print(pos_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0dda28",
   "metadata": {},
   "source": [
    "#Why Method 2 Is Better\n",
    "### ‚úÖ Method 2 Notes\n",
    "\n",
    "This approach is:\n",
    "- simpler\n",
    "- faster\n",
    "- more memory-efficient\n",
    "- standard practice when working with pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578700b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Pythonic one-liner (list comprehension)\n",
    "# List Comprehension (Optimized approach)\n",
    "records = [(t.text, t.pos_) for t in spacy_doc]\n",
    "pos_df = pd.DataFrame(records, columns=[\"token\", \"pos_tag\"])\n",
    "print(pos_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1300bed",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Counting Word Occurrences by POS Tag\n",
    "\n",
    "Next, we want to:\n",
    "- group similar words together\n",
    "- count how many times they appear\n",
    "- produce a clean summary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbf388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: groupby() + size()\n",
    "pos_df_counts = pos_df.groupby(['token','pos_tag']).size().reset_index(name ='counts').sort_values(by='counts', ascending=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaca1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc944ad8",
   "metadata": {},
   "source": [
    "### Conceptual Explanation (Bucket Model)\n",
    "\n",
    "- `groupby()` puts identical `(token, POS)` pairs into the same bucket\n",
    "- `.size()` counts how many items are inside each bucket\n",
    "- `reset_index()` converts bucket labels into normal columns\n",
    "- `sort_values()` shows the most frequent pairs first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa7de38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Using value_counts() (simpler)\n",
    "pos_df_counts_2 = (\n",
    "    pos_df\n",
    "    .value_counts(['token', 'pos_tag'])\n",
    "    .reset_index(name='counts')\n",
    "    .sort_values(by='counts', ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be067b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df_counts_2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68899dd3",
   "metadata": {},
   "source": [
    "### Counting how many words belong to each POS tag\n",
    "\n",
    "At this stage, `pos_df_counts` contains:\n",
    "- one row per unique `(token, pos_tag)` pair\n",
    "- a `counts` column showing how often each pair appears\n",
    "\n",
    "To understand the **distribution of parts of speech**, we now group the data **only by POS tag** and count how many different tokens fall under each tag.\n",
    "\n",
    "```python\n",
    "pos_df_poscounts = (\n",
    "    pos_df_counts\n",
    "    .groupby(['pos_tag'])['token']\n",
    "    .count()\n",
    "    .sort_values(ascending=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c873a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df_poscounts = pos_df_counts.groupby(['pos_tag'])['token'].count().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8a5631",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df_poscounts.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3f555e",
   "metadata": {},
   "source": [
    "### üîç Filtering the most common nouns\n",
    "\n",
    "To focus specifically on **nouns**, we filter the DataFrame to keep only rows where the POS tag is `NOUN`.\n",
    "\n",
    "```python\n",
    "nouns = pos_df_counts[pos_df_counts['pos_tag'] == 'NOUN']\n",
    "nouns.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b267b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = pos_df_counts[pos_df_counts['pos_tag'] == 'NOUN'] [:10]\n",
    "nouns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7ce8b7",
   "metadata": {},
   "source": [
    "### üîç Filtering the most common verbs\n",
    "\n",
    "To focus specifically on **verbs**, we filter the DataFrame to keep only rows where the POS tag is `NOUN`.\n",
    "\n",
    "```python\n",
    "nouns = pos_df_counts[pos_df_counts['pos_tag'] == 'NOUN']\n",
    "nouns.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d08f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = pos_df_counts[pos_df_counts['pos_tag'] == 'VERB'][:10]\n",
    "verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3858d0a7",
   "metadata": {},
   "source": [
    "## ‚úÖ Final Takeaways\n",
    "\n",
    "- spaCy Docs are already structured ‚Äî take advantage of the token-level information they provide\n",
    "- Collect data first, then create DataFrames in a single step\n",
    "- Avoid building DataFrames row by row inside loops\n",
    "- Use `value_counts()` when you only need frequency counts\n",
    "- Understanding *why* things work is more important than copying syntax\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
