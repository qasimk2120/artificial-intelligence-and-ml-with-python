{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c2732ef",
   "metadata": {},
   "source": [
    "# üì∞ POS and NER Practical Example on Realistic Text Data (NLP)\n",
    "\n",
    "This notebook demonstrates a **complete practical workflow** applying:\n",
    "\n",
    "- Text preprocessing\n",
    "- Part-of-Speech (POS) tagging\n",
    "- Named Entity Recognition (NER)\n",
    "\n",
    "on **real-world BBC news data** using:\n",
    "- spaCy\n",
    "- pandas\n",
    "- NLTK\n",
    "- regular expressions\n",
    "\n",
    "The goal is to move beyond toy examples and apply NLP techniques\n",
    "to **realistic, noisy text data**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc6b35d",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Overview of the Workflow\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "1. Load real BBC news data\n",
    "2. Focus on the **news titles** only\n",
    "3. Clean and preprocess text:\n",
    "   - lowercasing\n",
    "   - stopword removal\n",
    "   - punctuation removal\n",
    "   - tokenization\n",
    "   - lemmatization\n",
    "4. Apply **POS tagging** using spaCy\n",
    "5. Analyze most frequent nouns, verbs, and adjectives\n",
    "6. Apply **Named Entity Recognition (NER)**\n",
    "7. Visualize detected entities using displaCy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451ee8ae",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Importing Libraries\n",
    "\n",
    "We use a combination of NLP and data analysis libraries:\n",
    "\n",
    "- **NLTK** for tokenization, stopwords, and lemmatization\n",
    "- **spaCy** for POS tagging and NER\n",
    "- **pandas** for structured data manipulation\n",
    "- **regex** for text cleaning\n",
    "- **matplotlib** for optional visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8be9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from spacy import displacy \n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64450ba4",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Loading the BBC News Dataset\n",
    "\n",
    "We load BBC news data from a CSV file.\n",
    "Each row represents a news article, including a title and content.\n",
    "\n",
    "For this exercise, we focus only on the **title column**,\n",
    "which is short but rich in linguistic information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0d5950",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650edf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_data = pd.read_csv('../../data/bbc_news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bc2267",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beedf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_data.info()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670ebd59",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Selecting the Text for Analysis\n",
    "\n",
    "To simplify the analysis and reduce noise, we extract\n",
    "only the **news titles** into a separate DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7542b7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "titles = pd.DataFrame(bbc_data['title'])\n",
    "titles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b419a1e",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Text Cleaning and Preprocessing\n",
    "\n",
    "Before applying POS tagging and NER, we clean the text to improve consistency.\n",
    "\n",
    "Steps applied:\n",
    "- convert text to lowercase\n",
    "- remove stopwords\n",
    "- remove punctuation\n",
    "- tokenize text\n",
    "- lemmatize tokens\n",
    "\n",
    "This preprocessing helps reduce noise and normalize words\n",
    "before linguistic analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab2167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles['lowercase'] = titles['title'].str.lower()\n",
    "titles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bd3b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords = stopwords.words('english')\n",
    "titles['no_stopwords'] = titles['lowercase'].apply(lambda x: ' '. join([word for word in x.split() if word not in (en_stopwords)]))\n",
    "titles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccf4b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#punctuation removal\n",
    "titles['no_stopwords_nopunct'] = titles.apply(lambda x: re.sub(r\"[^\\w\\s]\", ' ', x['no_stopwords']), axis=1)\n",
    "titles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4efbe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "titles['tokens_raw'] = titles.apply(lambda x: word_tokenize(x['title']) , axis=1)\n",
    "titles['tokens_clean'] = titles.apply(lambda x: word_tokenize(x['no_stopwords_nopunct']) , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2ebf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "titles['tokens_clean_lemmatized'] = titles['tokens_clean'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])\n",
    "titles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f0a8aa",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Preparing Tokens for spaCy\n",
    "\n",
    "spaCy expects text input as a string, not a list of lists.\n",
    "\n",
    "After tokenizing and lemmatizing each title, we:\n",
    "- flatten all token lists\n",
    "- join them into a single string\n",
    "\n",
    "This allows spaCy to process the entire dataset in one pass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9f730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_raw_list = sum(titles['tokens_raw'], [])   \n",
    "tokens_clean_list = sum(titles['tokens_clean_lemmatized'], []) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbee725b",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Part-of-Speech (POS) Tagging\n",
    "\n",
    "Using spaCy, we assign a grammatical role\n",
    "(noun, verb, adjective, etc.) to each token.\n",
    "\n",
    "The result is stored in a pandas DataFrame with:\n",
    "- the token\n",
    "- its POS tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10fb474",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5925ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_doc = nlp(' '.join(tokens_clean_list))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc474c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [(t.text, t.pos_) for t in spacy_doc]\n",
    "pos_df = pd.DataFrame(records, columns=['token', 'pos_tag'])\n",
    "pos_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6508f5b",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Analyzing POS Frequencies\n",
    "\n",
    "We group tokens by:\n",
    "- word\n",
    "- POS tag\n",
    "\n",
    "and count how often each combination appears.\n",
    "\n",
    "This allows us to:\n",
    "- identify common nouns\n",
    "- identify frequent verbs\n",
    "- identify descriptive adjectives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840c5dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df_counts = pos_df.value_counts(['token', 'pos_tag']).reset_index(name='counts')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc31189",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df_counts.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1ef0cc",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Extracting Top POS Categories\n",
    "\n",
    "From the POS frequency table, we extract:\n",
    "- the most common nouns\n",
    "- the most common verbs\n",
    "- the most common adjectives\n",
    "\n",
    "This provides insight into the **language style**\n",
    "of BBC news headlines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa411bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = pos_df_counts[pos_df_counts['pos_tag'] == 'NOUN'][0:10]\n",
    "nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd9f2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = pos_df_counts[pos_df_counts['pos_tag'] == 'VERB'][0:10]\n",
    "verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3d65ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjectives = pos_df_counts[pos_df_counts['pos_tag'] == 'ADJ'][0:10]\n",
    "adjectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3a92b9",
   "metadata": {},
   "source": [
    "## üîü Named Entity Recognition (NER)\n",
    "\n",
    "Next, we apply **Named Entity Recognition** to detect:\n",
    "- people\n",
    "- organizations\n",
    "- locations\n",
    "- dates\n",
    "- geopolitical entities\n",
    "\n",
    "spaCy automatically identifies these entities\n",
    "based on context and linguistic cues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3c7b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for ent in spacy_doc.ents:\n",
    "    records.append((ent.text, ent.label_))\n",
    "ner_df = pd.DataFrame(records, columns=['token', 'ner_tag'])\n",
    "ner_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b5c3a7",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Counting Named Entities\n",
    "\n",
    "We count how often each named entity appears\n",
    "and group them by:\n",
    "- entity text\n",
    "- entity label\n",
    "\n",
    "This helps identify:\n",
    "- frequently mentioned organizations\n",
    "- recurring locations\n",
    "- prominent people in the news\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248c2bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_df_counts = ner_df.value_counts(['token', 'ner_tag']).reset_index(name='counts')\n",
    "ner_df_counts.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561768f9",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Visualizing Named Entities with displaCy\n",
    "\n",
    "Finally, we use **displaCy** to visually inspect\n",
    "the named entities detected by spaCy.\n",
    "\n",
    "Entities are highlighted directly in the text\n",
    "using different colors for each label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb76fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "html = displacy.render(spacy_doc, style=\"ent\", jupyter=False)  \n",
    "display(HTML(html))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6ea152",
   "metadata": {},
   "source": [
    "## ‚úÖ Final Takeaways\n",
    "\n",
    "- Real-world text requires careful preprocessing\n",
    "- POS tagging reveals grammatical structure\n",
    "- NER extracts meaningful real-world information\n",
    "- spaCy makes advanced NLP tasks accessible\n",
    "- Visualization helps validate model behavior\n",
    "- Understanding the pipeline is more important than copying code\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
