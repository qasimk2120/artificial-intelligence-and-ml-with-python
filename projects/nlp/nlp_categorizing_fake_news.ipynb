{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12ed989d",
   "metadata": {},
   "source": [
    "# üì∞ Categorizing Fake News Using NLP\n",
    "\n",
    "*(AI Engineer Course ‚Äì Applied NLP Project)*\n",
    "\n",
    "This notebook demonstrates an **end-to-end Natural Language Processing (NLP) pipeline**\n",
    "for categorizing news articles as **Fake** or **Factual**.\n",
    "\n",
    "The project combines:\n",
    "- Linguistic analysis\n",
    "- Text preprocessing\n",
    "- Statistical analysis\n",
    "- Topic modeling\n",
    "- Sentiment analysis\n",
    "- Supervised machine learning\n",
    "\n",
    "The focus is on understanding **how language patterns differ** between fake and factual news\n",
    "and how NLP techniques can be combined to build effective text classification systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0450a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy import tokenizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models import LsiModel, TfidfModel\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74aca58",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Data Loading and Exploration\n",
    "\n",
    "We begin by loading the dataset and exploring its structure.\n",
    "\n",
    "This step helps us:\n",
    "- Understand available columns\n",
    "- Inspect data types and missing values\n",
    "- Preview sample articles\n",
    "- Examine the distribution of fake vs factual news\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b245cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting plot options\n",
    "plt.rcParams['figure.figsize'] = (12,8)\n",
    "default_plot_colour = '#00bfbf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6568c3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../data/fake_news_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c278b7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b247eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf5c24d",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Distribution of Fake vs Factual News\n",
    "\n",
    "Before applying NLP techniques, it is important to understand\n",
    "the balance between classes in the dataset.\n",
    "\n",
    "We visualize the number of fake and factual articles to:\n",
    "- Detect class imbalance\n",
    "- Set expectations for model performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fd37d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['fake_or_factual'].value_counts().plot(kind='bar', color=default_plot_colour)\n",
    "plt.title('Distribution of Fake vs Factual News Articles')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fdad1f",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Linguistic Analysis Using spaCy\n",
    "\n",
    "spaCy is used to extract linguistic features from text, including:\n",
    "- Part-of-speech (POS) tags\n",
    "- Named entity labels\n",
    "\n",
    "Articles are processed separately for fake and factual news\n",
    "to compare language usage patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea51497e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp =  spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41ea0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news = data[data['fake_or_factual']=='Fake News']\n",
    "factual_news = data[data['fake_or_factual']=='Factual News']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f622aa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_spacydocs = list(nlp.pipe(fake_news['text']))\n",
    "factual_spacydocs = list(nlp.pipe(factual_news['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeeb2b3a",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Part-of-Speech (POS) Tag Analysis\n",
    "\n",
    "Part-of-speech tagging assigns grammatical roles such as:\n",
    "- Nouns\n",
    "- Verbs\n",
    "- Adjectives\n",
    "\n",
    "By comparing POS tag frequencies between fake and factual news,\n",
    "we can observe stylistic and structural differences in writing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e855a941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_token_tags(doc):\n",
    "    return [(i.text, i.ent_type_, i.pos_) for i in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ab514a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_tagsdf = []\n",
    "columns = ['token', \"ner_tag\", \"pos_tag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12c2613",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [{'token': token.text, 'ner_tag': token.ent_type_, 'pos_tag': token.pos_} for doc in fake_spacydocs for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574cc474",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_tagsdf = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d828c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_tagsdf = []\n",
    "rows = [{'token': token.text, 'ner_tag': token.ent_type_, 'pos_tag': token.pos_} for doc in factual_spacydocs for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d8656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_tagsdf = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11962ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_counts_fake = fake_tagsdf.value_counts(['token','pos_tag']).reset_index(name='counts')\n",
    "pos_counts_fake.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146c78bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_counts_fact = fact_tagsdf.value_counts(['token','pos_tag']).reset_index(name='counts')\n",
    "pos_counts_fact.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba8adce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_counts_fake['pos_tag'].value_counts().head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1352406",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_counts_fact['pos_tag'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eca35a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_counts_fake[pos_counts_fake.pos_tag == 'NOUN'][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39654729",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_counts_fact[pos_counts_fact.pos_tag == 'NOUN'][:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee832f9",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Named Entity Recognition (NER)\n",
    "\n",
    "Named Entity Recognition identifies references to:\n",
    "- People\n",
    "- Organizations\n",
    "- Locations\n",
    "- Dates and quantities\n",
    "\n",
    "In this section:\n",
    "- Named entities are extracted from both classes\n",
    "- The most common entities are compared\n",
    "- Entity frequencies are visualized for interpretation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f089094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull out all named entities before starting preprocessing\n",
    "top_entities_fake = fake_tagsdf[fake_tagsdf['ner_tag'] != ''].value_counts(['token','ner_tag']).reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e59476",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_entities_fact = fact_tagsdf[fact_tagsdf['ner_tag'] != ''].value_counts(['token','ner_tag']).reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0724993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_palette = {\n",
    "    'ORG': sns.color_palette(\"Set2\").as_hex()[0],\n",
    "    'GPE': sns.color_palette(\"Set2\").as_hex()[1],\n",
    "    'NORP': sns.color_palette(\"Set2\").as_hex()[2],\n",
    "    'PERSON': sns.color_palette(\"Set2\").as_hex()[3],\n",
    "    'DATE': sns.color_palette(\"Set2\").as_hex()[4],\n",
    "    'CARDINAL': sns.color_palette(\"Set2\").as_hex()[5],\n",
    "    'PERCENT': sns.color_palette(\"Set2\").as_hex()[6]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715d314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(\n",
    "    x = 'counts',\n",
    "    y = 'token',\n",
    "    hue='ner_tag',\n",
    "    palette=ner_palette,\n",
    "    data = top_entities_fake[:10],\n",
    "    orient= 'h',\n",
    "    dodge=False\n",
    ").set_title('Top Common Named Entities in Fake News Articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8a1f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(\n",
    "    x = 'counts',\n",
    "    y = 'token',\n",
    "    hue='ner_tag',\n",
    "    palette=ner_palette,\n",
    "    data = top_entities_fact[:10],\n",
    "    orient= 'h',\n",
    "    dodge=False\n",
    ").set_title('Top Common Named Entities in Factual News Articles')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998fa693",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Text Preprocessing\n",
    "\n",
    "Raw text must be cleaned and standardized before modeling.\n",
    "\n",
    "The preprocessing steps applied include:\n",
    "- Removing metadata and prefixes\n",
    "- Lowercasing text\n",
    "- Removing punctuation\n",
    "- Stopword removal\n",
    "- Tokenization\n",
    "- Lemmatization\n",
    "\n",
    "These steps reduce noise and prepare the text for analysis and modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f81ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf6b5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_clean'] =data.apply(lambda x: re.sub(r\"^[^-]*-\\s\", '', x['text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af3e22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_clean'] = data['text_clean'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f30946",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_clean'] = data.apply(lambda x: re.sub(r\"([^\\w\\s])\", '', x['text_clean']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c99d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "en_stopwards =  stopwords.words('english')\n",
    "print(en_stopwards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4509c5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_clean'] =  data['text_clean'].apply(lambda x: ' '.join ([word for word in x.split() if word not in en_stopwards]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723cf852",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_clean'] = data.apply(lambda x: word_tokenize(x['text_clean']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c274bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "data['text_clean'] = data['text_clean'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c44438",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700c9b9f",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ N-gram Analysis\n",
    "\n",
    "N-grams represent sequences of words:\n",
    "- Unigrams (single words)\n",
    "- Bigrams (pairs of words)\n",
    "\n",
    "This analysis helps identify:\n",
    "- Frequently used terms\n",
    "- Common word combinations\n",
    "- Dominant language patterns in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad93c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most common N grams\n",
    "tokens_clean = sum(data['text_clean'], []) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bdf43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = (pd.Series(nltk.ngrams(tokens_clean, 1)).value_counts()).reset_index(name='count')\n",
    "print(unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d220973",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams['tokens'] =  unigrams['index'].apply(lambda x: x[0])\n",
    "\n",
    "sns.barplot(x='count',\n",
    "            y='tokens',\n",
    "            data=unigrams[:15],\n",
    "            orient='h',\n",
    "            palette= [default_plot_colour],\n",
    "            hue='tokens', legend=False).set_title('Most Common Unigrams in News Articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09bd6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = (pd.Series(nltk.ngrams(tokens_clean, 2)).value_counts()).reset_index(name='count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aeaf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams['tokens'] =  bigrams['index'].apply(lambda x: x[0])\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603426df",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Sentiment Analysis\n",
    "\n",
    "Sentiment analysis measures the emotional tone of text.\n",
    "\n",
    "Using the VADER sentiment analyzer:\n",
    "- Each article receives a compound sentiment score\n",
    "- Scores are categorized as negative, neutral, or positive\n",
    "- Sentiment distributions are compared across fake and factual news\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59a81d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_sentiment = SentimentIntensityAnalyzer()\n",
    "data['vader_sentiment'] = data['text'].apply(lambda x: vader_sentiment.polarity_scores(x)['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844d67db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bda6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [-1, -0.01, 0.1, 1]\n",
    "names= ['negative', 'neutral', 'positive']\n",
    "data['vader_sentiment_label'] = pd.cut(data['vader_sentiment'], bins, labels=names)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe6bc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['vader_sentiment_label'].value_counts().plot(kind='bar', color=default_plot_colour)\n",
    "plt.title('Distribution of VADER Sentiment Labels in News Articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ece5d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(\n",
    "    x ='fake_or_factual',\n",
    "    data=data, \n",
    "    palette= sns.color_palette(\"hls\"), \n",
    "    hue= 'vader_sentiment_label'\n",
    "    ).set_title('VADER Sentiment Labels by Fake vs Factual News Articles')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9de6a61",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Topic Modeling\n",
    "\n",
    "Topic modeling uncovers hidden thematic structures in text data.\n",
    "\n",
    "Two approaches are used:\n",
    "- Latent Dirichlet Allocation (LDA)\n",
    "- Latent Semantic Indexing (LSI)\n",
    "\n",
    "Multiple topic counts are evaluated using coherence scores\n",
    "to identify meaningful topic representations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97cd319",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_text = data[data['fake_or_factual']=='Fake News']['text_clean'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f16f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_fake = corpora.Dictionary(fake_news_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8ad39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_fake =  [dictionary_fake.doc2bow(text) for text in fake_news_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d94777",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_values = []\n",
    "model_list = []\n",
    "min_topics = 2\n",
    "max_topics = 11\n",
    "\n",
    "for num_topics_i in range (min_topics, max_topics+1):\n",
    "    model =  gensim.models.LdaModel(doc_term_fake, num_topics=num_topics_i, id2word=dictionary_fake)\n",
    "    model_list.append(model)\n",
    "    coherence_model = CoherenceModel(model=model, texts=fake_news_text, dictionary=dictionary_fake, coherence='c_v')\n",
    "    coherence_values.append(coherence_model.get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da4c860",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(min_topics, max_topics+1), coherence_values)\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.ylabel('Coherence Score')\n",
    "plt.legend(('Coherence Values'), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57628e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics_lda = 7\n",
    "lda_model = gensim.models.LdaModel(doc_term_fake, num_topics=num_topics_lda, id2word=dictionary_fake)\n",
    "lda_model.print_topics(num_topics=num_topics_lda, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2026b8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tdidf_corpus(doc_term_matrix):\n",
    "    tdfidf = TfidfModel(corpus=doc_term_matrix, normalize=True)\n",
    "    corpus_tfidf = tdfidf[doc_term_matrix]\n",
    "    return corpus_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d2ffcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coherence_scores_lsi(corpus,dictionary,min_topics=2,max_topics=11):\n",
    "    corpus = list(corpus)\n",
    "    coherence_values = []\n",
    "    topic_range = range(min_topics, max_topics + 1)\n",
    "\n",
    "    for k in topic_range:\n",
    "        print(f\"Training LSI model with {k} topics\")\n",
    "        lsi_model = LsiModel(corpus=corpus,num_topics=k,id2word=dictionary)\n",
    "        coherence_model = CoherenceModel(model=lsi_model,corpus=corpus,dictionary=dictionary,coherence='u_mass')\n",
    "        coherence = coherence_model.get_coherence()\n",
    "        coherence_values.append(coherence)\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(topic_range, coherence_values, marker='o')\n",
    "    plt.xlabel(\"Number of Topics\")\n",
    "    plt.ylabel(\"Coherence Score (u_mass)\")\n",
    "    plt.title(\"LSI Topic Coherence (u_mass)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    # return coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2d71ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tdidf_fake = tdidf_corpus(doc_term_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78287d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf_fake = tdidf_corpus(doc_term_fake)\n",
    "coherence_scores = get_coherence_scores_lsi(\n",
    "    corpus=corpus_tfidf_fake,\n",
    "    dictionary=dictionary_fake,\n",
    "    min_topics=2,\n",
    "    max_topics=11\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4b2b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_model =  LsiModel(corpus_tdidf_fake, num_topics=3, id2word=dictionary_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1f42f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfe1f05",
   "metadata": {},
   "source": [
    "## üîü Text Classification\n",
    "\n",
    "In the final stage, supervised machine learning models are trained\n",
    "to classify articles as Fake or Factual.\n",
    "\n",
    "Steps include:\n",
    "- Vectorizing text using Bag-of-Words\n",
    "- Splitting data into training and test sets\n",
    "- Training classification models\n",
    "- Evaluating performance using accuracy and classification reports\n",
    "\n",
    "Models used:\n",
    "- Logistic Regression\n",
    "- Linear Support Vector Machine (SGDClassifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929f494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad7dd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [','.join(map(str, tokens)) for tokens in data['text_clean']]  #map list of tokens back to string for vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0751adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y= data['fake_or_factual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3114665",
   "metadata": {},
   "outputs": [],
   "source": [
    "countvec = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a417de99",
   "metadata": {},
   "outputs": [],
   "source": [
    "countvec_fit = countvec.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1c2a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = pd.DataFrame(countvec_fit.toarray(), columns=countvec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70524f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(bag_of_words,Y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32537005",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=0).fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221a26de",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lr = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0c3cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_pred_lr, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327f58f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccd1f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SGDClassifier(random_state=0).fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073a50fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svm = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42693dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_pred_svm, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9565e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83ca81a",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- Fake and factual news exhibit distinct linguistic patterns\n",
    "- Proper preprocessing significantly improves model performance\n",
    "- Topic modeling reveals hidden thematic differences\n",
    "- Sentiment alone is not sufficient but adds useful context\n",
    "- Classical ML models perform well when combined with NLP features\n",
    "\n",
    "This project demonstrates how multiple NLP techniques can be\n",
    "integrated into a complete text classification workflow.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
